{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03. DistilBERT Base Uncased.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5_hByr9-Asu"
      },
      "source": [
        "> **DISTILBERT BASE UNCASED**\r\n",
        ">\r\n",
        "> ---\r\n",
        ">\r\n",
        "> In this notebook we develop a neural model starting from a pre-trained **DistilBERT** model. We tokenize `paragraphs` and `questions` using the default tokenizer, then use the pretrained model as first layer for the neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgAW3FJSaiZ6"
      },
      "source": [
        "# **0. Preliminary Settings**\r\n",
        "\r\n",
        "At first, we need to clone the repository to get access to the code and use utility functions inside the notebook. The `src` folder is then added to the system path so that the modules can be used inside the notebook.\r\n",
        "\r\n",
        "Then, we use the utility functions in the `src` folder to get the ***train*** and ***validation*** splits, while we discard the ***test*** split as it will be used to evaluate the best model only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_fTKyb3S8_7"
      },
      "source": [
        "!git clone https://github.com/giuluck/Gangster-SQuAD\r\n",
        "\r\n",
        "import sys\r\n",
        "sys.path.append('Gangster-SQuAD/src')\r\n",
        "\r\n",
        "from dataset import get_dataframes\r\n",
        "train_df, val_df, _ = get_dataframes('Gangster-SQuAD/data/training_set.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iufmsB7_leO6"
      },
      "source": [
        "## TODO: remove\r\n",
        "train_df = train_df.iloc[:]\r\n",
        "val_df = val_df.iloc[:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49Oyo_iYbUXC"
      },
      "source": [
        "# **1. Preprocessing**\r\n",
        "\r\n",
        "We use **HuggingFace** to start from a pretrained **DistilBERT** model with its own vocabulary and tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwjHUNHCBqwM"
      },
      "source": [
        "%%capture\r\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9fGH4MhB-u-"
      },
      "source": [
        "### ***1.1. Tokenization***\r\n",
        "\r\n",
        "**HuggingFace** provides a pretrained *DistilBertTokenizer*, which is quite slow, and a faster *BertWordPieceTokenizer*. In order to exploit both of them, we initially load the pretrained tokenizer, store its data, and use that data to build the faster tokenizer.\r\n",
        "\r\n",
        "Once the tokenizer has been built, we use it to process every record in the dataframe in order to build the dataset used for training and testing purposes. This dataset will be composed by:\r\n",
        "- a **list of tokens** structured in this way $$[\\mbox{CLS}, \\mbox{ctx_tok}_0, ..., \\mbox{ctx_tok}_i, ..., \\mbox{ctx_tok}_n, \\mbox{SEP}, \\mbox{qst_tok}_0, ..., \\mbox{qst_tok}_j, ..., \\mbox{qst_tok}_m, \\mbox{SEP}]$$ which will be used as input for the *DistilBERT* model\r\n",
        "- a **start** and an **end** integer value representing the indices of the boundary tokens that identify the answer in the text, which will be used as outputs for the *DistilBERT* model\r\n",
        "- the **original context** and a **list of indices** representing the offsets, expressed in number of *chars* and not in number of *tokens*, which will be used to retrieve the original part of text in the context given the two outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fnPnDTl3a8a"
      },
      "source": [
        "class ModelInfo():\r\n",
        "  def __init__(self, pretrained_model, embedding_dim, max_length, cls_token, sep_token):\r\n",
        "    self.pretrained_model = pretrained_model\r\n",
        "    self.embedding_dim = embedding_dim\r\n",
        "    self.max_length = max_length\r\n",
        "    self.cls_token = cls_token\r\n",
        "    self.sep_token = sep_token\r\n",
        "\r\n",
        "model_info = ModelInfo('distilbert-base-uncased', embedding_dim=768, max_length=512, cls_token=101, sep_token=102)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pPh7k4zCH0T"
      },
      "source": [
        "from transformers import DistilBertTokenizer\r\n",
        "from tokenizers import BertWordPieceTokenizer\r\n",
        "\r\n",
        "DistilBertTokenizer.from_pretrained(model_info.pretrained_model).save_pretrained('slow_tokenizer/')\r\n",
        "tokenizer = BertWordPieceTokenizer('slow_tokenizer/vocab.txt', lowercase=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doW5C2l1CgYf"
      },
      "source": [
        "import pandas as pd\r\n",
        "from preprocessing import compute_boundaries\r\n",
        "\r\n",
        "def process_dataframe(df):\r\n",
        "  def process_record(record):\r\n",
        "    # both context and question gets tokenized\r\n",
        "    ctx_tokens = tokenizer.encode(record['context'])\r\n",
        "    ctx_ids = ctx_tokens.ids[1:-1]                       # [CLS] and [SEP] tokens are discarded\r\n",
        "    qst_tokens = tokenizer.encode(record['question'])\r\n",
        "    qst_ids = qst_tokens.ids[1:-1]                       # [CLS] and [SEP] tokens are discarded\r\n",
        "    # take all the context start chars then add a final index for the last character\r\n",
        "    offsets = [s for s, _ in ctx_tokens.offsets[:-1]] + [len(record['context'])]\r\n",
        "    # token boundaries to be used during training are computed\r\n",
        "    start_token, end_token = compute_boundaries(offsets, record['start'], len(record['answer']))\r\n",
        "    # input, output and utility data are returned to form the dataset\r\n",
        "    return [ctx_ids, qst_ids, start_token, end_token, offsets]\r\n",
        "\r\n",
        "  processed_df = pd.DataFrame(\r\n",
        "    [[id] + process_record(record) for id, record in df.iterrows()],\r\n",
        "    columns = ['id', 'ctx_ids', 'qst_ids', 'start token', 'end token', 'offsets']\r\n",
        "  ).set_index(['id'])\r\n",
        "  return processed_df.join(df)\r\n",
        "\r\n",
        "train_df = process_dataframe(train_df)\r\n",
        "val_df = process_dataframe(val_df)\r\n",
        "\r\n",
        "train_df[['ctx_ids', 'qst_ids', 'start token', 'end token', 'offsets']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA0pSZb-5aHw"
      },
      "source": [
        "> **DistilBERT** can manage sequences of 512 tokens at most, including one *\\[CLS\\]* token and two *\\[SEP\\]* tokens in our case. Thus, we exclude from the training dataset those sentences which exceed this limit. Still, validation sentences can be longer, therefore we will crop them later in the dataset creation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGCflR9Izzd7"
      },
      "source": [
        "excluded_train_ids = set([\r\n",
        "  id for (id, ctx_ids), qst_ids in zip(train_df['ctx_ids'].items(), train_df['qst_ids'])\r\n",
        "     if len(ctx_ids) + len(qst_ids) > model_info.max_length - 3\r\n",
        "])\r\n",
        "\r\n",
        "train_df = train_df[~train_df.index.isin(excluded_train_ids)]\r\n",
        "train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJGhs8-JS7h7"
      },
      "source": [
        "### ***1.2. Correctness Check***\r\n",
        "\r\n",
        "Once the dataframe is ready, we check that whether our tokenization is good enough to let us retrieve the correct answers from the text or not.\r\n",
        "\r\n",
        "Answers are retrived by:\r\n",
        "1. getting the two `start` and `end` (token) boundaries that should be computed by the model\r\n",
        "2. converting them into a `start_char` and an `end_char` pair of indices, which represent the boundaries in the original context, using the `indices` list\r\n",
        "3. selecting the correct portion of the `context` using these two (char) boundaries and strip the obtained substring\r\n",
        "\r\n",
        "Some of the answers are not correct, but this is due to the fact that the answers given in the dataset contain substrings or variations of the words which are present in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C0zwk5JOBZS"
      },
      "source": [
        "from preprocessing import retrieve_answer, check_correctness\r\n",
        "\r\n",
        "def retrieving_procedure(rec):\r\n",
        "  return retrieve_answer(rec['start token'], rec['end token'], rec['offsets'], rec['context'])\r\n",
        "\r\n",
        "check_correctness(pd.concat((train_df, val_df)), retrieving_procedure)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEk_uh8wu4-s"
      },
      "source": [
        "### ***1.3. Dataset Creation***\r\n",
        "\r\n",
        "We can now prepare the dataset using **Torch** utils for data managing.\r\n",
        "\r\n",
        "* The `SquadDataset` class extends *Torch's Dataset* and allows to get input and output data from the dataframe in a lazy way\r\n",
        "> Note that we add the *masks* tensor, which is currently a tensor of ones, that is used by *DistilBERT* to identify which token has to be considered and which one has to be discarded. Indeed, when we will pad the sequences, we will concatenate some *zeros* to this *masks* tensor to represent the padding tokens.\r\n",
        "\r\n",
        "* The `DataLoader`, then, is used to create mini-batches from the dataset and, via the custom function, to pad these mini-batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owigq4r-PBIS"
      },
      "source": [
        "import torch\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "\r\n",
        "class SquadDataset(Dataset):\r\n",
        "  def __init__(self, dataframe, model_info):\r\n",
        "    self.dataframe = dataframe\r\n",
        "    self.max_len = model_info.max_length\r\n",
        "    self.cls_tok = model_info.cls_token\r\n",
        "    self.sep_tok = model_info.sep_token\r\n",
        "        \r\n",
        "  def __getitem__(self, index):\r\n",
        "    rec = self.dataframe.iloc[index]\r\n",
        "    # retrieving paragraph and question tokens, limiting them to the maximal length\r\n",
        "    qst_ids = rec['qst_ids'][:self.max_len-3] + [self.sep_tok]\r\n",
        "    ctx_ids = [self.cls_tok] + rec['ctx_ids'][:self.max_len-len(qst_ids)-2] + [self.sep_tok]\r\n",
        "    len_ids = len(ctx_ids) + len(qst_ids)\r\n",
        "    # contexts and questions are used to build the input tensor\r\n",
        "    ctx_ids = torch.tensor(ctx_ids)\r\n",
        "    qst_ids = torch.tensor(qst_ids)\r\n",
        "    input_ids = torch.cat((ctx_ids, qst_ids))\r\n",
        "    input_masks = torch.ones_like(input_ids)\r\n",
        "    input_tensor = torch.stack((input_ids, input_masks), dim=0)\r\n",
        "    # the input tensor is padded to length 512\r\n",
        "    pad_tensor = torch.zeros((2, self.max_len - len_ids), dtype=torch.long)\r\n",
        "    input_tensor = torch.cat((input_tensor, pad_tensor), dim=1)\r\n",
        "    # an output tensor containing the two outputs is created as well\r\n",
        "    output_tensor = torch.tensor([rec['start token'], rec['end token']])\r\n",
        "    return input_tensor, output_tensor\r\n",
        "  \r\n",
        "  def __len__(self):\r\n",
        "    return len(self.dataframe)\r\n",
        "\r\n",
        "train_data = SquadDataset(train_df, model_info)\r\n",
        "val_data = SquadDataset(val_df, model_info)\r\n",
        "\r\n",
        "input, output = train_data[0]\r\n",
        "print('TRAIN DATA')\r\n",
        "print('Input:', input.shape)\r\n",
        "print('  > ids:', input[0].shape)\r\n",
        "print('  > types:', input[1].shape)\r\n",
        "print('Output:', output.shape)\r\n",
        "print('  > start:', output[0].shape)\r\n",
        "print('  > end:', output[1].shape)\r\n",
        "\r\n",
        "print()\r\n",
        "\r\n",
        "input, output = val_data[0]\r\n",
        "print('VAL DATA')\r\n",
        "print('Input:', input.shape)\r\n",
        "print('  > ids:', input[0].shape)\r\n",
        "print('  > types:', input[1].shape)\r\n",
        "print('Output:', output.shape)\r\n",
        "print('  > start:', output[0].shape)\r\n",
        "print('  > end:', output[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqXB-9JVSktD"
      },
      "source": [
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\r\n",
        "val_loader = DataLoader(val_data, batch_size=16, num_workers=4, pin_memory=True)\r\n",
        "\r\n",
        "for input, output in train_loader:\r\n",
        "  print('Input:', input.shape)\r\n",
        "  print('  > ids:', input[:, 0].shape)\r\n",
        "  print('  > types:', input[:, 1].shape)\r\n",
        "  print('Output:', output.shape)\r\n",
        "  print('  > start:', output[:, 0].shape)\r\n",
        "  print('  > end:', output[:, 1].shape)\r\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl86RM_oQDQL"
      },
      "source": [
        "# **2. Neural Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9_L2817QYPG"
      },
      "source": [
        "**DistilBERT** is a language model and we will use it as an encoder to produce contextual embeddings for our tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywtmi35V6NyI"
      },
      "source": [
        "%%capture\r\n",
        "!pip install wandb pytorch-lightning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HO7R8pCQFgB"
      },
      "source": [
        "### ***2.1. Baseline***\r\n",
        "\r\n",
        "> In the Baseline the model returns a dictionary with *one output*, the `last_hidden_state`, which has shape $[\\mbox{batch_size}, \\mbox{sequence_length}, \\mbox{embedding_dimension}]$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h6LrGZlJfQy"
      },
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import DistilBertModel\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class DistilBertCategorical(pl.LightningModule):\n",
        "    def __init__(self, model_info=model_info, alpha=0.5, alpha_step=0):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.alpha_step = alpha_step\n",
        "        self.encoder = DistilBertModel.from_pretrained(model_info.pretrained_model)\n",
        "        self.start_fc = nn.Linear(model_info.embedding_dim, 1)\n",
        "        self.end_fc = nn.Linear(model_info.embedding_dim, 1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def _logits(self, x):\n",
        "        x = self.encoder(input_ids=x[:, 0], attention_mask=x[:, 1])\n",
        "        x = x[\"last_hidden_state\"]\n",
        "        start = self.start_fc(x).squeeze(dim=2)\n",
        "        end = self.end_fc(x).squeeze(dim=2)\n",
        "        return start, end\n",
        "\n",
        "    def forward(self, x):\n",
        "        start, end = self._logits(x)\n",
        "        prob_start = self.softmax(start)\n",
        "        prob_end = self.softmax(end)\n",
        "        _, start_indices = prob_start.max(dim=1)\n",
        "        _, end_indices = prob_end.max(dim=1)\n",
        "\n",
        "        return start_indices, end_indices\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        pred_start, pred_end = self._logits(x)\n",
        "        loss_start = self.criterion(pred_start, y[:, 0])\n",
        "        loss_end = self.criterion(pred_end, y[:, 1])\n",
        "        self.log('loss_start', loss_start, prog_bar=True)\n",
        "        self.log('loss_end', loss_end, prog_bar=True)\n",
        "        self.log('alpha', self.alpha, prog_bar=True)\n",
        "        loss = self.alpha * loss_start + (1 - self.alpha) * loss_end\n",
        "        if loss_end > loss_start:\n",
        "            self.alpha -= self.alpha_step \n",
        "        else:\n",
        "            self.alpha += self.alpha_step\n",
        "        \n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self.training_step(batch, batch_idx)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5, weight_decay=1e-12)\n",
        "        return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bopx0CraQjtz"
      },
      "source": [
        "### ***2.2 DistilBERT with dot product attention over Hidden Layers and Highway***\r\n",
        "\r\n",
        "> The base model is focused on the `last_hidden_state` which is passed through two linear layers and a softmax function to obtain `prob_start` and `prob_end`. The reason behind this choice lies in the most informative content of the last representation. However this approach is an approximation because we would be excluding a priori the additional information contained in the previous `hidden_states`. With this extended model \"we aim to combine\r\n",
        "hidden states from previous time steps by weighting them to generate an overall representation that may be able to capture additional semantic and positional patterns\" according to Takeuchi et al. In essence the last `k` `hidden_states` are concatenated, hence the output of the model become $[\\mbox{batch_size}, \\mbox{sequence_length}, \\mbox{embedding_dimension}, \\mbox{k}]$ and it is passed through a linear layer which weight the `hidden_states` and output a tensor of size $[\\mbox{batch_size}, \\mbox{sequence_length}, \\mbox{embedding_dimension}]$\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU-34vrePCbH"
      },
      "source": [
        "> The **Highway** network can be used before computing logits and it must improve the performances because it is able to filter the irrelevant information. The highway model computes:\r\n",
        "\r\n",
        "*   $x\\_proj = ReLU(W\\_proj_x + b\\_proj)$\r\n",
        "*   $x\\_gate = σ(W\\_gate_x + b\\_gate)$\r\n",
        "\r\n",
        "> It returns: $x\\_highway = x\\_gate ⊙ x\\_proj + (1 − x\\_gate) ⊙ x$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DGRmzrOO5XE"
      },
      "source": [
        "class Highway(nn.Module):\r\n",
        "    def __init__(self, in_size, n_layers=1, act=F.relu):\r\n",
        "        super(Highway, self).__init__()\r\n",
        "        self.n_layers = n_layers\r\n",
        "        self.act = act\r\n",
        "\r\n",
        "        self.normal_layer = nn.ModuleList([nn.Linear(in_size, in_size) for _ in range(n_layers)])\r\n",
        "        self.gate_layer = nn.ModuleList([nn.Linear(in_size, in_size) for _ in range(n_layers)])\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        for i in range(self.n_layers):\r\n",
        "            normal_layer_ret = self.act(self.normal_layer[i](x))\r\n",
        "            gate = torch.sigmoid(self.gate_layer[i](x))\r\n",
        "\r\n",
        "            x = torch.add(torch.mul(normal_layer_ret, gate),torch.mul((1.0 - gate), x))\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J8p55yt9bf-"
      },
      "source": [
        "# https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/default/15737384.pdf\r\n",
        "\r\n",
        "class DistilBertCategoricalWHL(DistilBertCategorical):\r\n",
        "    def __init__(self, model_info=model_info, k=4, alpha=0.5, alpha_step=0, highway=False):\r\n",
        "        super(DistilBertCategoricalWHL, self).__init__(model_info, alpha, alpha_step)\r\n",
        "        self.k = k\r\n",
        "        self.highway = highway\r\n",
        "        if highway:\r\n",
        "            self.highway = Highway(in_size=model_info.embedding_dim * k)\r\n",
        "        self.dot_prod_attention = nn.Linear(k * model_info.embedding_dim, model_info.embedding_dim)\r\n",
        "\r\n",
        "\r\n",
        "    def _logits(self, x):\r\n",
        "        x = self.encoder(input_ids=x[:, 0], attention_mask=x[:, 1], output_hidden_states=True)\r\n",
        "        x_hidden_states = x[\"hidden_states\"]\r\n",
        "        x = torch.stack([x_hidden_state for x_hidden_state in x_hidden_states[-self.k:]], dim=3)\r\n",
        "        x = x.flatten(start_dim = 2, end_dim = 3)\r\n",
        "        if self.highway:\r\n",
        "            x = self.highway(x)\r\n",
        "        x = self.dot_prod_attention(x)\r\n",
        "        start = self.start_fc(x).squeeze(dim=2)\r\n",
        "        end = self.end_fc(x).squeeze(dim=2)\r\n",
        "        return start, end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddcHlHvj28pX"
      },
      "source": [
        "### ***2.3 DistilBERT with CNN***\r\n",
        "> With this extension we would like to achieve a similar goal of the **DistilBERT with dot product attention over Hidden Layers**: aggregate multiple informations of different hidden layers to obtain a more complete representation layer. For this purpose we use a CNN which takes an input with dimension $[\\mbox{batch_size}, \\mbox{embedding_dimension}, \\mbox{sequence_length}, \\mbox{4}]$ and outputs a tensor with dimension $[\\mbox{batch_size}, \\mbox{embedding_dimension}, \\mbox{sequence_length}]$. The output is reshaped  so that it can be correctly taken as input from the two linear layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-TL30Vc3Cpv"
      },
      "source": [
        "class DistilBertCategoricalCNN(DistilBertCategorical):\r\n",
        "    def __init__(self, model_info=model_info, alpha=0.5, alpha_step=0):\r\n",
        "        super(DistilBertCategoricalCNN, self).__init__(model_info, alpha, alpha_step)\r\n",
        "        self.conv = nn.Conv2d(in_channels=model_info.embedding_dim, out_channels=model_info.embedding_dim, kernel_size=(1, 2), stride=(1, 4))\r\n",
        "    \r\n",
        "    def _logits(self, x):\r\n",
        "        x = self.encoder(input_ids=x[:, 0], attention_mask=x[:, 1], output_hidden_states=True)\r\n",
        "        x_hidden_states = x[\"hidden_states\"]\r\n",
        "        x = torch.stack([x_hidden_state for x_hidden_state in x_hidden_states[-4:]], dim=3)\r\n",
        "        x = x.transpose(1, 2)\r\n",
        "        x = self.conv(x).squeeze(dim=3)\r\n",
        "        x = x.transpose(1, 2)\r\n",
        "        start = self.start_fc(x).squeeze(dim=2)\r\n",
        "        end = self.end_fc(x).squeeze(dim=2)\r\n",
        "        return start, end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl5sFBIUOrTq"
      },
      "source": [
        "# **3. Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA_foo5qOr-y"
      },
      "source": [
        "import wandb\r\n",
        "from pytorch_lightning.loggers import WandbLogger\r\n",
        "\r\n",
        "# Access to W&B\r\n",
        "wandb.login()\r\n",
        "\r\n",
        "wandb_project_name = \"SQuAD\"\r\n",
        "# Automatically assigned if None, duplicates are automatically handled\r\n",
        "wandb_run_name = None\r\n",
        "# Entity\r\n",
        "wandb_entity = 'lomb'\r\n",
        "\r\n",
        "# Create the logger\r\n",
        "wandb_logger = WandbLogger(project=\"SQuAD\", entity=wandb_entity, name=wandb_run_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ee35GIoO7HR"
      },
      "source": [
        "model = DistilBertCategorical(alpha=0.66, alpha_step=0.0001)\r\n",
        "\r\n",
        "trainer = pl.Trainer(\r\n",
        "    logger=wandb_logger,\r\n",
        "    max_epochs=4, precision=16,\r\n",
        "    gpus=-1, distributed_backend='ddp',\r\n",
        "    checkpoint_callback=False\r\n",
        ")\r\n",
        "\r\n",
        "trainer.fit(model, train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9VE-M-tPECe"
      },
      "source": [
        "# Run this to stop the synchronization with the previous run\r\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5K2fdu5C9IJ"
      },
      "source": [
        "# **4. Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhPx4SaMDCli"
      },
      "source": [
        "starts, ends = [], []\r\n",
        "num_batches = len(val_loader)\r\n",
        "\r\n",
        "# pytorch lightning does not move the model to GPU during evaluation\r\n",
        "# so we need to do that by hand for both the model and the inputs\r\n",
        "model = model.cuda()\r\n",
        "model.eval()\r\n",
        "\r\n",
        "print('Starting Evaluation...')\r\n",
        "for idx, (input, _) in enumerate(val_loader):\r\n",
        "  if (idx + 1) % 100 == 0:\r\n",
        "    print(f'Batch {idx+1:{len(str(num_batches))}}/{num_batches}')\r\n",
        "  with torch.no_grad(): \r\n",
        "    s, e = model(input.cuda())\r\n",
        "  starts.append(s)\r\n",
        "  ends.append(e)\r\n",
        "\r\n",
        "val_df['pred_start'] = [s.item() for ss in starts for s in ss]\r\n",
        "val_df['pred_end'] = [e.item() for ee in ends for e in ee]\r\n",
        "# When the prediction (for both 'start' and 'end') points to a padding character \r\n",
        "# then it is placed at the last offset\r\n",
        "val_df['length_offsets'] = val_df['offsets'].apply(lambda x: len(x))\r\n",
        "val_df['pred_start'] = (val_df['pred_start'] < val_df['length_offsets']) * val_df['pred_start'] + (val_df['pred_start'] >= val_df['length_offsets']) * (val_df['length_offsets'] - 1)\r\n",
        "val_df['pred_end'] = (val_df['pred_end'] < val_df['length_offsets']) * val_df['pred_end'] + (val_df['pred_end'] >= val_df['length_offsets']) * (val_df['length_offsets'] - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geFuojMkDDFc"
      },
      "source": [
        "from metrics import compute_metrics\r\n",
        "\r\n",
        "def retrieving_procedure(rec):\r\n",
        "  return retrieve_answer(rec['pred_start'], rec['pred_end'], rec['offsets'], rec['context'])\r\n",
        "\r\n",
        "exact_match, f1 = compute_metrics(val_df, retrieving_procedure)\r\n",
        "print(\"EM: {}\\nF1: {}\".format(exact_match, f1))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}