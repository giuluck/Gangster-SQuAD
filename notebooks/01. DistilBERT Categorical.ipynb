{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01. DistilBERT Categorical.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgAW3FJSaiZ6"
      },
      "source": [
        "# **0. Preliminary Settings**\r\n",
        "\r\n",
        "At first, we need to clone the repository to get access to the code and use utility functions inside the notebook. The `src` folder is then added to the system path so that the modules can be used inside the notebook.\r\n",
        "\r\n",
        "Then, we use the utility functions in the `src` folder to get the ***train*** and ***validation*** splits, while we discard the ***test*** split as it will be used to evaluate the best model only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwjHUNHCBqwM"
      },
      "source": [
        "%%capture\r\n",
        "!pip install transformers wandb pytorch-lightning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_fTKyb3S8_7"
      },
      "source": [
        "!git clone https://github.com/giuluck/Gangster-SQuAD\r\n",
        "\r\n",
        "import sys\r\n",
        "sys.path.append('Gangster-SQuAD')\r\n",
        "sys.path.append('Gangster-SQuAD/src')\r\n",
        "sys.path.append('Gangster-SQuAD/src/models')\r\n",
        "\r\n",
        "from dataframe import get_dataframes\r\n",
        "train_df, val_df, _ = get_dataframes('Gangster-SQuAD/data/training_set.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iufmsB7_leO6"
      },
      "source": [
        "## TODO: remove\r\n",
        "train_df = train_df.iloc[:1000]\r\n",
        "val_df = val_df.iloc[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49Oyo_iYbUXC"
      },
      "source": [
        "# **1. Preprocessing**\r\n",
        "\r\n",
        "We use **HuggingFace** to start from a pretrained **DistilBERT** model with its own vocabulary and tokenizer.\r\n",
        "\r\n",
        "**HuggingFace** provides a pretrained *DistilBertTokenizer*, which is quite slow, and a faster *BertWordPieceTokenizer*. In order to exploit both of them, we initially load the pretrained tokenizer, store its data, and use that data to build the faster tokenizer.\r\n",
        "\r\n",
        "Once the tokenizer has been built, we use it to process every record in the dataframe in order to build the dataset used for training and testing purposes. This dataset will be composed by:\r\n",
        "- a **list of tokens** structured in this way $$[\\mbox{CLS}, \\mbox{ctx_tok}_0, ..., \\mbox{ctx_tok}_i, ..., \\mbox{ctx_tok}_n, \\mbox{SEP}, \\mbox{qst_tok}_0, ..., \\mbox{qst_tok}_j, ..., \\mbox{qst_tok}_m, \\mbox{SEP}]$$ which will be used as input for the *DistilBERT* model\r\n",
        "- a **start** and an **end** integer value representing the indices of the boundary tokens that identify the answer in the text, which will be used as outputs for the *DistilBERT* model\r\n",
        "- the **original context** and a **list of indices** representing the offsets, expressed in number of *chars* and not in number of *tokens*, which will be used to retrieve the original part of text in the context given the two outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fnPnDTl3a8a"
      },
      "source": [
        "from models import ModelInfo\r\n",
        "\r\n",
        "model_info = ModelInfo('distilbert-base-uncased', embedding_dim=768, max_length=512, cls_token=101, sep_token=102)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pPh7k4zCH0T"
      },
      "source": [
        "from transformers import DistilBertTokenizer\r\n",
        "from tokenizers import BertWordPieceTokenizer\r\n",
        "\r\n",
        "DistilBertTokenizer.from_pretrained(model_info.pretrained_model).save_pretrained('slow_tokenizer/')\r\n",
        "tokenizer = BertWordPieceTokenizer('slow_tokenizer/vocab.txt', lowercase=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doW5C2l1CgYf"
      },
      "source": [
        "import pandas as pd\r\n",
        "from preprocessing import compute_boundaries\r\n",
        "from dataframe import process_dataframe\r\n",
        "\r\n",
        "train_df = process_dataframe(train_df, tokenizer)\r\n",
        "val_df = process_dataframe(val_df, tokenizer)\r\n",
        "\r\n",
        "train_df[['ctx_ids', 'qst_ids', 'start token', 'end token', 'offsets']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA0pSZb-5aHw"
      },
      "source": [
        "> **DistilBERT** can manage sequences of 512 tokens at most, including one *\\[CLS\\]* token and two *\\[SEP\\]* tokens in our case. Thus, we exclude from the training dataset those sentences which exceed this limit. Still, validation sentences can be longer, therefore we will crop them later in the dataset creation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGCflR9Izzd7"
      },
      "source": [
        "excluded_train_ids = set([\r\n",
        "  id for (id, ctx_ids), qst_ids in zip(train_df['ctx_ids'].items(), train_df['qst_ids'])\r\n",
        "     if len(ctx_ids) + len(qst_ids) > model_info.max_length - 3\r\n",
        "])\r\n",
        "\r\n",
        "train_df = train_df[~train_df.index.isin(excluded_train_ids)]\r\n",
        "train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJGhs8-JS7h7"
      },
      "source": [
        "> Once the dataframe is ready, we check that whether our tokenization is good enough to let us retrieve the correct answers from the text or not.\r\n",
        ">\r\n",
        "> Answers are retrived by:\r\n",
        "> 1. getting the two `start` and `end` (token) boundaries that should be computed by the model\r\n",
        "> 2. converting them into a `start_char` and an `end_char` pair of indices, which represent the boundaries in the original context, using the `indices` list\r\n",
        "> 3. selecting the correct portion of the `context` using these two (char) boundaries and strip the obtained substring\r\n",
        ">\r\n",
        "> Some of the answers are not correct, but this is due to the fact that the answers given in the dataset contain substrings or variations of the words which are present in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C0zwk5JOBZS"
      },
      "source": [
        "from preprocessing import retrieve_answer\r\n",
        "from dataframe import check_correctness\r\n",
        "\r\n",
        "def retrieving_procedure(rec):\r\n",
        "  return retrieve_answer(rec['start token'], rec['end token'], rec['offsets'], rec['context'])\r\n",
        "\r\n",
        "check_correctness(pd.concat((train_df, val_df)), retrieving_procedure)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEk_uh8wu4-s"
      },
      "source": [
        "# ***2. Dataset Creation***\r\n",
        "\r\n",
        "We can now prepare the dataset using **Torch** utils for data managing.\r\n",
        "\r\n",
        "* The `SquadDataset` class extends *Torch's Dataset* and allows to get input and output data from the dataframe in a lazy way\r\n",
        "> Note that we add the *masks* tensor, which is currently a tensor of ones, that is used by *DistilBERT* to identify which token has to be considered and which one has to be discarded. Indeed, when we will pad the sequences, we will concatenate some *zeros* to this *masks* tensor to represent the padding tokens.\r\n",
        "\r\n",
        "* The `DataLoader`, then, is used to create mini-batches from the dataset and, via the custom function, to pad these mini-batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owigq4r-PBIS"
      },
      "source": [
        "from dataset import SquadDataset\r\n",
        "\r\n",
        "train_data = SquadDataset(train_df, model_info)\r\n",
        "val_data = SquadDataset(val_df, model_info)\r\n",
        "\r\n",
        "input, output = train_data[0]\r\n",
        "print('TRAIN DATA')\r\n",
        "print('Input:', input.shape)\r\n",
        "print('  > ids:', input[0].shape)\r\n",
        "print('  > types:', input[1].shape)\r\n",
        "print('Output:', output.shape)\r\n",
        "print('  > start:', output[0].shape)\r\n",
        "print('  > end:', output[1].shape)\r\n",
        "\r\n",
        "print()\r\n",
        "\r\n",
        "input, output = val_data[0]\r\n",
        "print('VAL DATA')\r\n",
        "print('Input:', input.shape)\r\n",
        "print('  > ids:', input[0].shape)\r\n",
        "print('  > types:', input[1].shape)\r\n",
        "print('Output:', output.shape)\r\n",
        "print('  > start:', output[0].shape)\r\n",
        "print('  > end:', output[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqXB-9JVSktD"
      },
      "source": [
        "from torch.utils.data import DataLoader\r\n",
        "\r\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\r\n",
        "val_loader = DataLoader(val_data, batch_size=16, num_workers=4, pin_memory=True)\r\n",
        "\r\n",
        "for input, output in train_loader:\r\n",
        "  print('Input:', input.shape)\r\n",
        "  print('  > ids:', input[:, 0].shape)\r\n",
        "  print('  > types:', input[:, 1].shape)\r\n",
        "  print('Output:', output.shape)\r\n",
        "  print('  > start:', output[:, 0].shape)\r\n",
        "  print('  > end:', output[:, 1].shape)\r\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl86RM_oQDQL"
      },
      "source": [
        "# **3. Neural Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9_L2817QYPG"
      },
      "source": [
        "**DistilBERT** returns the `last_hidden_state`, which has shape $[\\mbox{batch_size}, \\mbox{sequence_length}, \\mbox{embedding_dimension}]$. In this model, we simply pass that values to *two fully-connected layers* which will return the *logits* related to each token. The model is trained via **Categorical Cross-Entropy**, while the outputs of the model are the two values for the *start* and *end* token respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaYKThoU6BTe"
      },
      "source": [
        "> In the `forward` method, we return the indices for both the *start* and the *end* token. Then, we clip them to the maximal context lenght because, if the model gives a wrong token and returns a value which is greater than the length of the `offsets` list, an error will occurr."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h6LrGZlJfQy"
      },
      "source": [
        "from models import DistilBertBase\n",
        "\n",
        "model = DistilBertBase()\n",
        "save_model = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7z-EMvmwfiv"
      },
      "source": [
        "W&B keeps track of the metrics and the hyperparameters, login to the service and start training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCOy8Q57wCyb"
      },
      "source": [
        "import wandb\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "use_wandb = False\n",
        "\n",
        "if use_wandb:\n",
        "  # Access to W&B\n",
        "  wandb.login()\n",
        "  run_name = None\n",
        "  # Create the logger\n",
        "  wandb_logger = WandbLogger(project=\"SQuAD\", entity=\"lomb\", name=run_name)\n",
        "else:\n",
        "  wandb_logger = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtxxwxGR16ah"
      },
      "source": [
        "import pytorch_lightning as pl\r\n",
        "\r\n",
        "trainer = pl.Trainer(\r\n",
        "  logger=wandb_logger,\r\n",
        "  max_epochs=4, precision=16,\r\n",
        "  gpus=-1, distributed_backend='ddp',\r\n",
        "  checkpoint_callback=False\r\n",
        ")\r\n",
        "\r\n",
        "trainer.fit(model, train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuJ30dYA18ig"
      },
      "source": [
        "if use_wandb:\n",
        "  # Run this to stop the synchronization with the previous run\n",
        "  wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_9zgFBqnm6s"
      },
      "source": [
        "if save_model:\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/content/drive\")\n",
        "  \n",
        "  PATH = '/content/drive/My Drive/Colab Notebooks'\n",
        "  torch.save(model.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcKeeTeMCNES"
      },
      "source": [
        "# **4. Results**\r\n",
        "\r\n",
        "We now use the model to get the two boundaries for the answer. Then, we append these boundaries as two new columns in the *validation dataset* and use them to retrieve the textual answer inside the context paragraph. Finally, we get the dataset of wrong answers in order to insepct it, and from that we compute the **validation score** of the model as the ratio of good answers on the total number of validation samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcyTgSlp_uzz"
      },
      "source": [
        "starts, ends = [], []\n",
        "num_batches = len(val_loader)\n",
        "\n",
        "# pytorch lightning does not move the model to GPU during evaluation\n",
        "# so we need to do that by hand for both the model and the inputs\n",
        "model = model.cuda()\n",
        "\n",
        "print('Starting Evaluation...')\n",
        "for idx, (input, _) in enumerate(val_loader):\n",
        "  if (idx + 1) % 100 == 0:\n",
        "    print(f'Batch {idx+1:{len(str(num_batches))}}/{num_batches}')\n",
        "  s, e = model(input.cuda())\n",
        "  starts.append(s)\n",
        "  ends.append(e)\n",
        "\n",
        "val_df['pred_start'] = [s.item() for ss in starts for s in ss]\n",
        "val_df['pred_end'] = [e.item() for ee in ends for e in ee]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FjJZlp7xRHA"
      },
      "source": [
        "from metrics import compute_metrics\r\n",
        "\r\n",
        "def retrieving_procedure(rec):\r\n",
        "  return retrieve_answer(rec['pred_start'], rec['pred_end'], rec['offsets'], rec['context'])\r\n",
        "\r\n",
        "exact_match, f1 = compute_metrics(val_df, retrieving_procedure)\r\n",
        "print(\"EM: {}\\nF1: {}\".format(exact_match, f1))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}