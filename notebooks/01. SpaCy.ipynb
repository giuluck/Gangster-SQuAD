{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01. SpaCy.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3tOEic89H89"
      },
      "source": [
        "> **SPACY**\r\n",
        ">\r\n",
        "> ---\r\n",
        ">\r\n",
        "> In this notebook we develop a neural model leveraging **SpaCy**. We preprocess `paragraphs` and `questions` using the english language model in order to tokenize the sentences and retrieve their contextual embeddings. Finally, we use these embeddings as inputs for the model to be trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgAW3FJSaiZ6"
      },
      "source": [
        "# **0. Preliminary Settings**\r\n",
        "\r\n",
        "At first, we need to clone the repository to get access to the code and use utility functions inside the notebook. The `src` folder is then added to the system path so that the modules can be used inside the notebook.\r\n",
        "\r\n",
        "Then, we use the utility functions in the `src` folder to get the ***train*** and ***validation*** splits, while we discard the ***test*** split as it will be used to evaluate the best model only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLOWIhI2u8rz"
      },
      "source": [
        "!git clone https://github.com/giuluck/Gangster-SQuAD\r\n",
        "\r\n",
        "import sys\r\n",
        "sys.path.append('Gangster-SQuAD/src')\r\n",
        "\r\n",
        "from dataset import get_dataframes\r\n",
        "train_df, val_df, _ = get_dataframes('Gangster-SQuAD/data/training_set.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn13eRr3up2v"
      },
      "source": [
        "## TODO: remove\r\n",
        "train_df = train_df.iloc[:1000]\r\n",
        "val_df = val_df.iloc[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABWlhyiCoZ3S"
      },
      "source": [
        "# **1. Preprocessing**\r\n",
        "\r\n",
        "We use **SpaCy** to tokenize both the paragraphs and the questions and then extract the *contextual embedding* of each token. We also change a little bit the default tokenizer to be able to split consecutive punctuation characters and deal with strange dashes that are in the dataset.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ1XitX-12z9"
      },
      "source": [
        "%%capture\r\n",
        "!python -m spacy download en_core_web_md\r\n",
        "!python -m spacy link en_core_web_md en_core_web_md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdeeBAwEyM_d"
      },
      "source": [
        "import string\r\n",
        "import spacy\r\n",
        "from spacy.tokenizer import Tokenizer\r\n",
        "from spacy.util import compile_infix_regex\r\n",
        "\r\n",
        "nlp = spacy.load('en_core_web_md')\r\n",
        "\r\n",
        "split_chars = string.punctuation + '––'                            # weird dashes added\r\n",
        "inf = list(nlp.Defaults.infixes) + [f'(?<=.)[{split_chars}](?=.)'] # always split by punctuation chars\r\n",
        "infix_re = compile_infix_regex(tuple(inf))\r\n",
        "\r\n",
        "nlp.tokenizer = Tokenizer(\r\n",
        "  nlp.vocab,\r\n",
        "  prefix_search=nlp.tokenizer.prefix_search,\r\n",
        "  suffix_search=nlp.tokenizer.suffix_search,\r\n",
        "  infix_finditer=infix_re.finditer,\r\n",
        "  token_match=nlp.tokenizer.token_match,\r\n",
        "  rules=nlp.Defaults.tokenizer_exceptions\r\n",
        ")\r\n",
        "\r\n",
        "for token in nlp(\"SpaCy is cooler than NLTK, ain't it?\"):\r\n",
        "  print(f'{token.string:7}--> {len(token.tensor)} features')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYOGHhaxyFPh"
      },
      "source": [
        "### ***1.1. Tokenization***\r\n",
        "\r\n",
        "We process each sentences using *Spacy*.\r\n",
        "\r\n",
        "- **Questions** are (almost) unique in the dataset, so we process them individually and we get the matrix of *contextual embeddings* for each of them. This matrix is then appended to the respective record both in the *train* and in the *validation* dataset.\r\n",
        "\r\n",
        "- **Contexts**, instead, are repeated along records. Thus, as tokenizing a sentence is computationally heavy, in order to avoid to do that multiple times for the same sentences, we store the unique contexts in a set and then process them.\r\n",
        "  * differently than for *questions*, for *contexts* we do not retrieve just the matrix of *contextual embeddings* but also the list of char *offsets* that will be used for retrieving the answer in the initial context given the two token bounaried predicted by the neural model.\r\n",
        "  * as both of these data is heavy, we do not store them in the dataframe itself but rather in a list containing both the *embeddings* and the *offsets* of a certain *context*, so that in the dataframe we can just add a column for the respective index and avoid using memory to store the same information related to the same *contexts* multiple times.\r\n",
        "\r\n",
        "> Please note that this procedure should take around **20 minutes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bWL-Eg4u7_k"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "def process_sentences(sentences, extract_features, log_rate=1000):\r\n",
        "  output = []\r\n",
        "  for i, sentence in enumerate(sentences):\r\n",
        "    if log_rate > 0 and i % log_rate == 0:\r\n",
        "      format_chars = len(str(len(sentences) - 1))\r\n",
        "      print(f'Sentence {i:{format_chars}}/{len(sentences)}')\r\n",
        "    doc = nlp(sentence)\r\n",
        "    features = extract_features(doc)\r\n",
        "    output.append(features)\r\n",
        "  return output\r\n",
        "\r\n",
        "def extract_embeddings(doc):\r\n",
        "  return np.array([np.concatenate((tok.vector, tok.tensor)) for tok in doc])\r\n",
        "\r\n",
        "extract_embeddings(nlp(\"SpaCy is cooler than NLTK, ain't it?\")).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1UWppnE-Vkn"
      },
      "source": [
        "train_df['qst_embedding'] = process_sentences(train_df['question'], extract_features=extract_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVc3T6T6-hwq"
      },
      "source": [
        "val_df['qst_embedding'] = process_sentences(val_df['question'], extract_features=extract_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3duiQzLzSbd"
      },
      "source": [
        "import pandas as pd\r\n",
        "from preprocessing import get_offsets\r\n",
        "\r\n",
        "contexts = pd.concat((train_df['context'], val_df['context']))\r\n",
        "contexts = { ctx: idx for idx, ctx in enumerate(set(contexts)) }\r\n",
        "\r\n",
        "train_df['ctx_index'] = [contexts[ctx] for ctx in train_df['context']]\r\n",
        "val_df['ctx_index'] = [contexts[ctx] for ctx in val_df['context']]\r\n",
        "\r\n",
        "contexts = process_sentences(list(contexts.keys()), extract_features=lambda ctx: {\r\n",
        "  'ctx_embedding': extract_embeddings(ctx),\r\n",
        "  'offsets': get_offsets([token.string for token in ctx])\r\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU0tzYLgGLrp"
      },
      "source": [
        "### ***1.2. Boundaries Computation***\r\n",
        "\r\n",
        "Once the contexts have been tokenized, we can compute the boundaries to identify the answer inside the context, which will be used for training and validation purposes as they will be the **outputs** of the neural model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XohneEvF3ZY0"
      },
      "source": [
        "from preprocessing import compute_boundaries\r\n",
        "\r\n",
        "def add_boundaries(df):\r\n",
        "  start_indices = []\r\n",
        "  end_indices = []\r\n",
        "  for _, record in df.iterrows():\r\n",
        "    ctx = contexts[record['ctx_index']]\r\n",
        "    start_idx, end_idx = compute_boundaries(ctx['offsets'], record['start'], len(record['answer']))\r\n",
        "    start_indices.append(start_idx)\r\n",
        "    end_indices.append(end_idx)\r\n",
        "  df['start_idx'] = start_indices\r\n",
        "  df['end_idx'] = end_indices\r\n",
        "\r\n",
        "add_boundaries(train_df)\r\n",
        "add_boundaries(val_df)\r\n",
        "\r\n",
        "train_df[['start_idx', 'end_idx']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFosYV2b46Gz"
      },
      "source": [
        "### ***1.3. Correctness Check***\r\n",
        "\r\n",
        "Once the dataframe is ready, we check that whether our tokenization is good enough to let us retrieve the correct answers from the text or not.\r\n",
        "\r\n",
        "Answers are retrived by:\r\n",
        "1. getting the two `start` and `end` (token) boundaries that should be computed by the model\r\n",
        "2. converting them into a `start_char` and an `end_char` pair of indices, which represent the boundaries in the original context, using the `indices` list\r\n",
        "3. selecting the correct portion of the `context` using these two (char) boundaries and strip the obtained substring\r\n",
        "\r\n",
        "Some of the answers are not correct, but this is due to the fact that the answers given in the dataset contain substrings or variations of the words which are present in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "763EC2EV4-F3"
      },
      "source": [
        "from preprocessing import retrieve_answer, check_correctness\r\n",
        "\r\n",
        "def retrieving_procedure(rec):\r\n",
        "  ctx = contexts[rec['ctx_index']]\r\n",
        "  return retrieve_answer(rec['start_idx'], rec['end_idx'], ctx['offsets'], rec['context'])\r\n",
        "\r\n",
        "check_correctness(pd.concat((train_df, val_df)), retrieving_procedure)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9Vh_kq0rCBj"
      },
      "source": [
        "### ***1.4. Dataset Creation***\r\n",
        "\r\n",
        "We can now prepare the dataset using **Torch** utils for data managing.\r\n",
        "\r\n",
        "* The `SquadDataset` class extends *Torch's Dataset* and allows to get input and output data from the dataframe in a lazy way\r\n",
        "* The `DataLoader`, then, is used to create mini-batches from the dataset and, via the custom function, to pad these mini-batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8bh4lfIhXy_"
      },
      "source": [
        "import torch\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "\r\n",
        "class SquadDataset(Dataset):\r\n",
        "  def __init__(self, dataframe):\r\n",
        "    ctx_lengths = dataframe['ctx_index'].map(lambda idx: len(contexts[idx]['ctx_embedding']))\r\n",
        "    qst_lengths = dataframe['qst_embedding'].map(lambda emb: len(emb))\r\n",
        "    self.max_len = max([cl + ql for cl, ql in zip(ctx_lengths, qst_lengths)]) + 1 # +1 for the sep token\r\n",
        "    self.dataframe = dataframe\r\n",
        "        \r\n",
        "  def __getitem__(self, index):\r\n",
        "    rec = self.dataframe.iloc[index]\r\n",
        "    ctx = contexts[rec['ctx_index']]\r\n",
        "    input_ctx = torch.tensor(ctx['ctx_embedding'])\r\n",
        "    input_qst = torch.tensor(rec['qst_embedding'])\r\n",
        "    input_sep = torch.zeros((1, input_ctx.shape[1]))\r\n",
        "    input_len = len(input_ctx) + len(input_qst) + 1\r\n",
        "    input_pad = torch.zeros((self.max_len - input_len, input_ctx.shape[1]))\r\n",
        "    input = torch.cat((input_ctx, input_sep, input_qst, input_pad))\r\n",
        "    output = torch.tensor([rec['start_idx'], rec['end_idx']])\r\n",
        "    return (input, input_len), output\r\n",
        "  \r\n",
        "  def __len__(self):\r\n",
        "    return len(self.dataframe)\r\n",
        "\r\n",
        "train_data = SquadDataset(train_df)\r\n",
        "val_data = SquadDataset(val_df)\r\n",
        "\r\n",
        "(input, _), output = train_data[0]\r\n",
        "print('Input:', input.shape)\r\n",
        "print('Output:', output.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm0HP9rJA2L7"
      },
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence\r\n",
        "\r\n",
        "def collate_fn(samples):\r\n",
        "  inputs = torch.stack([input for (input, _), _ in samples])\r\n",
        "  lengths = torch.tensor([length for (_, length), _ in samples])\r\n",
        "  outputs = torch.stack([output for _, output in samples])\r\n",
        "  return pack_padded_sequence(inputs, lengths, batch_first=True, enforce_sorted=False), outputs\r\n",
        "\r\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate_fn)\r\n",
        "val_loader = DataLoader(val_data, batch_size=64, shuffle=False, collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGahW6MBN5iM"
      },
      "source": [
        "# **2. Neural Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeHP2pouYeTt"
      },
      "source": [
        "%%capture\r\n",
        "!pip install pytorch-lightning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WOGkqrXLvkB"
      },
      "source": [
        "from torch import nn\r\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\r\n",
        "import pytorch_lightning as pl\r\n",
        "\r\n",
        "class SpacyCategorical(pl.LightningModule):\r\n",
        "    def __init__(self, embedding_dim=396, hidden_dim=128, alpha=0.5):\r\n",
        "        super().__init__()\r\n",
        "        self.alpha = alpha\r\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\r\n",
        "        self.start_fc = nn.Linear(2 * hidden_dim, 1)\r\n",
        "        self.end_fc = nn.Linear(2 * hidden_dim, 1)\r\n",
        "        self.softmax = nn.Softmax(dim=1)\r\n",
        "        self.criterion = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "    def _logits(self, x):\r\n",
        "        x, _ = self.rnn(x)\r\n",
        "        x, _ = pad_packed_sequence(x, batch_first=True)\r\n",
        "        start = self.start_fc(x).squeeze(dim=2)\r\n",
        "        end = self.end_fc(x).squeeze(dim=2)\r\n",
        "        return start, end\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        start, end = self._logits(x)\r\n",
        "        prob_start = self.softmax(start)\r\n",
        "        prob_end = self.softmax(end)\r\n",
        "        return prob_start, prob_end\r\n",
        "    \r\n",
        "    def _step(self, batch, batch_idx):\r\n",
        "        x, y = batch\r\n",
        "        pred_start, pred_end = self._logits(x)\r\n",
        "        loss_start = self.criterion(pred_start, y[:, 0])\r\n",
        "        loss_end = self.criterion(pred_end, y[:, 1])\r\n",
        "        return loss_start, loss_end\r\n",
        "\r\n",
        "    def training_step(self, batch, batch_idx):\r\n",
        "        loss_start, loss_end = self._step(batch, batch_idx)\r\n",
        "        loss = self.alpha * loss_start + (1 - self.alpha) * loss_end\r\n",
        "        self.log('loss_start', loss_start, prog_bar=True)\r\n",
        "        self.log('loss_end', loss_end, prog_bar=True)\r\n",
        "        return loss\r\n",
        "\r\n",
        "    def validation_step(self, batch, batch_idx):\r\n",
        "        loss_start, loss_end = self._step(batch, batch_idx)\r\n",
        "        loss = self.alpha * loss_start + (1 - self.alpha) * loss_end\r\n",
        "        self.log('val_loss_start', loss_start, prog_bar=True)\r\n",
        "        self.log('val_loss_end', loss_end, prog_bar=True)\r\n",
        "        self.log('val_loss', loss, prog_bar=True)\r\n",
        "        return loss\r\n",
        "\r\n",
        "    def configure_optimizers(self):\r\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\r\n",
        "        return optimizer\r\n",
        "\r\n",
        "spacy_categorical = SpacyCategorical()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMF7nKA7XPZJ"
      },
      "source": [
        "from pytorch_lightning.callbacks import EarlyStopping\r\n",
        "\r\n",
        "trainer = pl.Trainer(\r\n",
        "  max_epochs=10, gpus=-1, progress_bar_refresh_rate=20, logger=False, checkpoint_callback=False,\r\n",
        "  callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=5)]\r\n",
        ")\r\n",
        "\r\n",
        "trainer.fit(spacy_categorical, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ylQHn8pptTn"
      },
      "source": [
        "starts, ends = [], []\r\n",
        "for input, output in val_loader:\r\n",
        "  s, e = spacy_categorical(input)\r\n",
        "  _, s = s.max(dim=1)\r\n",
        "  _, e = e.max(dim=1)\r\n",
        "  starts.append(s.numpy())\r\n",
        "  ends.append(e.numpy())\r\n",
        "\r\n",
        "val_df['pred_start'] = [s for ss in starts for s in ss]\r\n",
        "val_df['pred_end'] = [e for ee in ends for e in ee]\r\n",
        "val_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6EX7MxQzvlt"
      },
      "source": [
        "def retrieving_procedure(rec):\r\n",
        "  ctx = contexts[rec['ctx_index']]\r\n",
        "  max_len = len(ctx['offsets']) - 1\r\n",
        "  pred_start = min(rec['pred_start'], max_len)\r\n",
        "  pred_end = min(rec['pred_end'], max_len)\r\n",
        "  return retrieve_answer(pred_start, pred_end, ctx['offsets'], rec['context'])\r\n",
        "\r\n",
        "wrong_answers = check_correctness(val_df, retrieving_procedure)\r\n",
        "wrong_answers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L35UL0sC0F_I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}