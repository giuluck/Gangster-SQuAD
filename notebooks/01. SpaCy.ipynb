{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01. SpaCy.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Q3tOEic89H89"},"source":["> **SPACY**\r\n",">\r\n","> ---\r\n",">\r\n","> In this notebook we develop a neural model leveraging **SpaCy**. We preprocess `paragraphs` and `questions` using the english language model in order to tokenize the sentences and retrieve their contextual embeddings. Finally, we use these embeddings as inputs for the model to be trained."]},{"cell_type":"markdown","metadata":{"id":"DgAW3FJSaiZ6"},"source":["# **0. Preliminary Settings**\r\n","\r\n","At first, we need to clone the repository to get access to the code and use utility functions inside the notebook. The `src` folder is then added to the system path so that the modules can be used inside the notebook.\r\n","\r\n","Then, we use the utility functions in the `src` folder to get the ***train*** and ***validation*** splits, while we discard the ***test*** split as it will be used to evaluate the best model only."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oLOWIhI2u8rz","executionInfo":{"status":"ok","timestamp":1610409241098,"user_tz":-60,"elapsed":5029,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"6b24888a-7ed9-488e-d009-540b93649e29"},"source":["!git clone https://github.com/giuluck/Gangster-SQuAD\r\n","\r\n","import sys\r\n","sys.path.append('Gangster-SQuAD/src')\r\n","\r\n","from dataset import get_dataframes\r\n","train_df, val_df, _ = get_dataframes('Gangster-SQuAD/data/training_set.json')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'Gangster-SQuAD'...\n","remote: Enumerating objects: 40, done.\u001b[K\n","remote: Counting objects: 100% (40/40), done.\u001b[K\n","remote: Compressing objects: 100% (29/29), done.\u001b[K\n","remote: Total 40 (delta 12), reused 31 (delta 6), pack-reused 0\u001b[K\n","Unpacking objects: 100% (40/40), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bn13eRr3up2v"},"source":["## TODO: remove\r\n","train_df = train_df.iloc[:1000]\r\n","val_df = val_df.iloc[:100]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ABWlhyiCoZ3S"},"source":["# **1. Preprocessing**\r\n","\r\n","We use **SpaCy** to tokenize both the paragraphs and the questions and then extract the *contextual embedding* of each token. We also change a little bit the default tokenizer to be able to split consecutive punctuation characters and deal with strange dashes that are in the dataset.\r\n","\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NdeeBAwEyM_d","executionInfo":{"status":"ok","timestamp":1610409243181,"user_tz":-60,"elapsed":7098,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"90674c60-4d81-4149-8078-68db344e305b"},"source":["import string\r\n","import spacy\r\n","from spacy.tokenizer import Tokenizer\r\n","from spacy.util import compile_infix_regex\r\n","\r\n","nlp = spacy.load('en_core_web_sm')\r\n","\r\n","split_chars = string.punctuation + '––'                            # weird dashes added\r\n","inf = list(nlp.Defaults.infixes) + [f'(?<=.)[{split_chars}](?=.)'] # always split by punctuation chars\r\n","infix_re = compile_infix_regex(tuple(inf))\r\n","\r\n","nlp.tokenizer = Tokenizer(\r\n","  nlp.vocab,\r\n","  prefix_search=nlp.tokenizer.prefix_search,\r\n","  suffix_search=nlp.tokenizer.suffix_search,\r\n","  infix_finditer=infix_re.finditer,\r\n","  token_match=nlp.tokenizer.token_match,\r\n","  rules=nlp.Defaults.tokenizer_exceptions\r\n",")\r\n","\r\n","for token in nlp(\"SpaCy is cooler than NLTK, ain't it?\"):\r\n","  print(f'{token.string:7}--> {len(token.tensor)} features')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["SpaCy  --> 96 features\n","is     --> 96 features\n","cooler --> 96 features\n","than   --> 96 features\n","NLTK   --> 96 features\n",",      --> 96 features\n","ai     --> 96 features\n","n't    --> 96 features\n","it     --> 96 features\n","?      --> 96 features\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IYOGHhaxyFPh"},"source":["### ***1.1. Tokenization***\r\n","\r\n","We process each sentences using *Spacy*.\r\n","\r\n","- **Questions** are (almost) unique in the dataset, so we process them individually and we get the matrix of *contextual embeddings* for each of them. This matrix is then appended to the respective record both in the *train* and in the *validation* dataset.\r\n","\r\n","- **Contexts**, instead, are repeated along records. Thus, as tokenizing a sentence is computationally heavy, in order to avoid to do that multiple times for the same sentences, we store the unique contexts in a set and then process them.\r\n","  * differently than for *questions*, for *contexts* we do not retrieve just the matrix of *contextual embeddings* but also the list of char *offsets* that will be used for retrieving the answer in the initial context given the two token bounaried predicted by the neural model.\r\n","  * as both of these data is heavy, we do not store them in the dataframe itself but rather in a list containing both the *embeddings* and the *offsets* of a certain *context*, so that in the dataframe we can just add a column for the respective index and avoid using memory to store the same information related to the same *contexts* multiple times.\r\n","\r\n","> Please note that this procedure should take around **20 minutes**"]},{"cell_type":"code","metadata":{"id":"1bWL-Eg4u7_k"},"source":["import numpy as np\r\n","\r\n","def process_sentences(sentences, extract_features, log_rate=1000):\r\n","  output = []\r\n","  for i, sentence in enumerate(sentences):\r\n","    if log_rate > 0 and i % log_rate == 0:\r\n","      format_chars = len(str(len(sentences) - 1))\r\n","      print(f'Sentence {i:{format_chars}}/{len(sentences)}')\r\n","    doc = nlp(sentence)\r\n","    features = extract_features(doc)\r\n","    output.append(features)\r\n","  return output\r\n","\r\n","def extract_embeddings(doc):\r\n","  return doc.tensor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b1UWppnE-Vkn","executionInfo":{"status":"ok","timestamp":1610409908993,"user_tz":-60,"elapsed":672891,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"74f6031c-3e6a-4f9a-9b3e-f9cd079126b1"},"source":["train_df['qst_embedding'] = process_sentences(train_df['question'], extract_features=extract_embeddings)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sentence     0/49084\n","Sentence  1000/49084\n","Sentence  2000/49084\n","Sentence  3000/49084\n","Sentence  4000/49084\n","Sentence  5000/49084\n","Sentence  6000/49084\n","Sentence  7000/49084\n","Sentence  8000/49084\n","Sentence  9000/49084\n","Sentence 10000/49084\n","Sentence 11000/49084\n","Sentence 12000/49084\n","Sentence 13000/49084\n","Sentence 14000/49084\n","Sentence 15000/49084\n","Sentence 16000/49084\n","Sentence 17000/49084\n","Sentence 18000/49084\n","Sentence 19000/49084\n","Sentence 20000/49084\n","Sentence 21000/49084\n","Sentence 22000/49084\n","Sentence 23000/49084\n","Sentence 24000/49084\n","Sentence 25000/49084\n","Sentence 26000/49084\n","Sentence 27000/49084\n","Sentence 28000/49084\n","Sentence 29000/49084\n","Sentence 30000/49084\n","Sentence 31000/49084\n","Sentence 32000/49084\n","Sentence 33000/49084\n","Sentence 34000/49084\n","Sentence 35000/49084\n","Sentence 36000/49084\n","Sentence 37000/49084\n","Sentence 38000/49084\n","Sentence 39000/49084\n","Sentence 40000/49084\n","Sentence 41000/49084\n","Sentence 42000/49084\n","Sentence 43000/49084\n","Sentence 44000/49084\n","Sentence 45000/49084\n","Sentence 46000/49084\n","Sentence 47000/49084\n","Sentence 48000/49084\n","Sentence 49000/49084\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FVc3T6T6-hwq","executionInfo":{"status":"ok","timestamp":1610410131187,"user_tz":-60,"elapsed":895079,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"b7051579-7e84-493f-e36c-1381d92a3602"},"source":["val_df['qst_embedding'] = process_sentences(val_df['question'], extract_features=extract_embeddings)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sentence     0/16595\n","Sentence  1000/16595\n","Sentence  2000/16595\n","Sentence  3000/16595\n","Sentence  4000/16595\n","Sentence  5000/16595\n","Sentence  6000/16595\n","Sentence  7000/16595\n","Sentence  8000/16595\n","Sentence  9000/16595\n","Sentence 10000/16595\n","Sentence 11000/16595\n","Sentence 12000/16595\n","Sentence 13000/16595\n","Sentence 14000/16595\n","Sentence 15000/16595\n","Sentence 16000/16595\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a3duiQzLzSbd","executionInfo":{"status":"ok","timestamp":1610411044099,"user_tz":-60,"elapsed":1807984,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"e90b9069-abb1-4076-cee9-d17be9fdca2b"},"source":["import pandas as pd\r\n","from preprocessing import get_offsets\r\n","\r\n","contexts = pd.concat((train_df['context'], val_df['context']))\r\n","contexts = { ctx: idx for idx, ctx in enumerate(set(contexts)) }\r\n","\r\n","train_df['ctx_index'] = [contexts[ctx] for ctx in train_df['context']]\r\n","val_df['ctx_index'] = [contexts[ctx] for ctx in val_df['context']]\r\n","\r\n","contexts = process_sentences(list(contexts.keys()), extract_features=lambda ctx: {\r\n","  'ctx_embedding': extract_embeddings(ctx),\r\n","  'offsets': get_offsets([token.string for token in ctx])\r\n","})"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sentence     0/14245\n","Sentence  1000/14245\n","Sentence  2000/14245\n","Sentence  3000/14245\n","Sentence  4000/14245\n","Sentence  5000/14245\n","Sentence  6000/14245\n","Sentence  7000/14245\n","Sentence  8000/14245\n","Sentence  9000/14245\n","Sentence 10000/14245\n","Sentence 11000/14245\n","Sentence 12000/14245\n","Sentence 13000/14245\n","Sentence 14000/14245\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fU0tzYLgGLrp"},"source":["### ***1.2. Boundaries Computation***\r\n","\r\n","Once the contexts have been tokenized, we can compute the boundaries to identify the answer inside the context, which will be used for training and validation purposes as they will be the **outputs** of the neural model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"id":"XohneEvF3ZY0","executionInfo":{"status":"ok","timestamp":1610411052079,"user_tz":-60,"elapsed":1815955,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"f9467542-a827-499c-90f1-72731c36e858"},"source":["from preprocessing import compute_boundaries\r\n","\r\n","def add_boundaries(df):\r\n","  start_indices = []\r\n","  end_indices = []\r\n","  for _, record in df.iterrows():\r\n","    ctx = contexts[record['ctx_index']]\r\n","    start_idx, end_idx = compute_boundaries(ctx['offsets'], record['start'], len(record['answer']))\r\n","    start_indices.append(start_idx)\r\n","    end_indices.append(end_idx)\r\n","  df['start_idx'] = start_indices\r\n","  df['end_idx'] = end_indices\r\n","\r\n","add_boundaries(train_df)\r\n","add_boundaries(val_df)\r\n","\r\n","train_df[['start_idx', 'end_idx']]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>start_idx</th>\n","      <th>end_idx</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5733be284776f41900661182</th>\n","      <td>102</td>\n","      <td>105</td>\n","    </tr>\n","    <tr>\n","      <th>5733be284776f4190066117f</th>\n","      <td>37</td>\n","      <td>42</td>\n","    </tr>\n","    <tr>\n","      <th>5733be284776f41900661180</th>\n","      <td>57</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>5733be284776f41900661181</th>\n","      <td>76</td>\n","      <td>83</td>\n","    </tr>\n","    <tr>\n","      <th>5733be284776f4190066117e</th>\n","      <td>17</td>\n","      <td>24</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5726f9485951b619008f83db</th>\n","      <td>68</td>\n","      <td>73</td>\n","    </tr>\n","    <tr>\n","      <th>5726f9485951b619008f83dc</th>\n","      <td>113</td>\n","      <td>116</td>\n","    </tr>\n","    <tr>\n","      <th>5726f9485951b619008f83dd</th>\n","      <td>121</td>\n","      <td>123</td>\n","    </tr>\n","    <tr>\n","      <th>5726f9485951b619008f83de</th>\n","      <td>135</td>\n","      <td>137</td>\n","    </tr>\n","    <tr>\n","      <th>5726f9485951b619008f83df</th>\n","      <td>54</td>\n","      <td>56</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>49084 rows × 2 columns</p>\n","</div>"],"text/plain":["                          start_idx  end_idx\n","id                                          \n","5733be284776f41900661182        102      105\n","5733be284776f4190066117f         37       42\n","5733be284776f41900661180         57       60\n","5733be284776f41900661181         76       83\n","5733be284776f4190066117e         17       24\n","...                             ...      ...\n","5726f9485951b619008f83db         68       73\n","5726f9485951b619008f83dc        113      116\n","5726f9485951b619008f83dd        121      123\n","5726f9485951b619008f83de        135      137\n","5726f9485951b619008f83df         54       56\n","\n","[49084 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"hFosYV2b46Gz"},"source":["### ***1.3. Correctness Check***\r\n","\r\n","Once the dataframe is ready, we check that whether our tokenization is good enough to let us retrieve the correct answers from the text or not.\r\n","\r\n","Answers are retrived by:\r\n","1. getting the two `start` and `end` (token) boundaries that should be computed by the model\r\n","2. converting them into a `start_char` and an `end_char` pair of indices, which represent the boundaries in the original context, using the `indices` list\r\n","3. selecting the correct portion of the `context` using these two (char) boundaries and strip the obtained substring\r\n","\r\n","Some of the answers are not correct, but this is due to the fact that the answers given in the dataset contain substrings or variations of the words which are present in the text."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":490},"id":"763EC2EV4-F3","executionInfo":{"status":"ok","timestamp":1610411062923,"user_tz":-60,"elapsed":1826791,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"f2786c2e-b3a7-417e-dabb-0771ac7c0205"},"source":["from preprocessing import retrieve_answer, check_correctness\r\n","\r\n","def retrieving_procedure(rec):\r\n","  ctx = contexts[rec['ctx_index']]\r\n","  return retrieve_answer(rec['start_idx'], rec['end_idx'], ctx['offsets'], rec['context'])\r\n","\r\n","check_correctness(pd.concat((train_df, val_df)), retrieving_procedure)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>answer</th>\n","      <th>normalized answer</th>\n","      <th>retrieved</th>\n","      <th>normalized retrieved</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>56bf7e603aeaaa14008c9681</th>\n","      <td>split with Luckett and Rober</td>\n","      <td>split with luckett and rober</td>\n","      <td>split with Luckett and Roberson</td>\n","      <td>split with luckett and roberson</td>\n","    </tr>\n","    <tr>\n","      <th>56be973d3aeaaa14008c9123</th>\n","      <td>six</td>\n","      <td>six</td>\n","      <td>sixth</td>\n","      <td>sixth</td>\n","    </tr>\n","    <tr>\n","      <th>5733bc38d058e614000b6188</th>\n","      <td>evolution</td>\n","      <td>evolution</td>\n","      <td>evolutionary</td>\n","      <td>evolutionary</td>\n","    </tr>\n","    <tr>\n","      <th>56cbdea66d243a140015edae</th>\n","      <td>7</td>\n","      <td>7</td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>56cf609aaab44d1400b89187</th>\n","      <td>7</td>\n","      <td>7</td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>572aaec2111d821400f38cb3</th>\n","      <td>85 years after his great-grandmother, Queen Vi...</td>\n","      <td>85 years after his greatgrandmother queen vict...</td>\n","      <td>years after his great-grandmother, Queen Victo...</td>\n","      <td>years after his greatgrandmother queen victori...</td>\n","    </tr>\n","    <tr>\n","      <th>57280b532ca10214002d9c7e</th>\n","      <td>Some people with asthma rarely experience symp...</td>\n","      <td>some people with asthma rarely experience symp...</td>\n","      <td>Some people with asthma rarely experience symp...</td>\n","      <td>some people with asthma rarely experience symp...</td>\n","    </tr>\n","    <tr>\n","      <th>572822da3acd2414000df55f</th>\n","      <td>bilateral</td>\n","      <td>bilateral</td>\n","      <td>bilaterally</td>\n","      <td>bilaterally</td>\n","    </tr>\n","    <tr>\n","      <th>572bfa9ef182dd1900d7c7a3</th>\n","      <td>German</td>\n","      <td>german</td>\n","      <td>Germanic</td>\n","      <td>germanic</td>\n","    </tr>\n","    <tr>\n","      <th>5728b4714b864d1900164c70</th>\n","      <td>Joseph Bohm</td>\n","      <td>joseph bohm</td>\n","      <td>Joseph Bohmann</td>\n","      <td>joseph bohmann</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>257 rows × 4 columns</p>\n","</div>"],"text/plain":["                                                                     answer  ...                               normalized retrieved\n","id                                                                           ...                                                   \n","56bf7e603aeaaa14008c9681                       split with Luckett and Rober  ...                    split with luckett and roberson\n","56be973d3aeaaa14008c9123                                                six  ...                                              sixth\n","5733bc38d058e614000b6188                                          evolution  ...                                       evolutionary\n","56cbdea66d243a140015edae                                                  7  ...                                                   \n","56cf609aaab44d1400b89187                                                  7  ...                                                   \n","...                                                                     ...  ...                                                ...\n","572aaec2111d821400f38cb3  85 years after his great-grandmother, Queen Vi...  ...  years after his greatgrandmother queen victori...\n","57280b532ca10214002d9c7e  Some people with asthma rarely experience symp...  ...  some people with asthma rarely experience symp...\n","572822da3acd2414000df55f                                          bilateral  ...                                        bilaterally\n","572bfa9ef182dd1900d7c7a3                                             German  ...                                           germanic\n","5728b4714b864d1900164c70                                        Joseph Bohm  ...                                     joseph bohmann\n","\n","[257 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"l9Vh_kq0rCBj"},"source":["### ***1.4. Dataset Creation***\r\n","\r\n","We can now prepare the dataset using **Torch** utils for data managing.\r\n","\r\n","* The `Data` class extends *Torch's Dataset* and allows to get input and output data from the dataframe in a lazy way\r\n","* The `DataLoader`, then, is used to create mini-batches from the dataset and, via the custom function, to pad these mini-batches."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h10rll8EYHjp","executionInfo":{"status":"ok","timestamp":1610411065227,"user_tz":-60,"elapsed":1829088,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"745ccbcd-8a10-4c3e-bb44-bd8d8b4190f9"},"source":["import torch\r\n","from torch.utils.data import Dataset, DataLoader\r\n","\r\n","class Data(Dataset):\r\n","  def __init__(self, dataframe):\r\n","    self.dataframe = dataframe\r\n","        \r\n","  def __getitem__(self, index):\r\n","    rec = self.dataframe.iloc[index]\r\n","    ctx = contexts[rec['ctx_index']]\r\n","    input_ctx = torch.tensor(ctx['ctx_embedding'])\r\n","    input_qst = torch.tensor(rec['qst_embedding'])\r\n","    output_start = torch.tensor(rec['start_idx'])\r\n","    output_end = torch.tensor(rec['end_idx'])\r\n","    return (input_ctx, input_qst), (output_start, output_end)\r\n","  \r\n","  def __len__(self):\r\n","    return len(self.dataframe)\r\n","\r\n","train_data = Data(train_df)\r\n","val_data = Data(val_df)\r\n","\r\n","input, output = train_data[0]\r\n","print('Input:')\r\n","print('  > context:', input[0].shape)\r\n","print('  > question:', input[1].shape)\r\n","print('Output:')\r\n","print('  > start:', output[0].shape)\r\n","print('  > end:', output[1].shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input:\n","  > context: torch.Size([142, 96])\n","  > question: torch.Size([14, 96])\n","Output:\n","  > start: torch.Size([])\n","  > end: torch.Size([])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4gUGJ3-DfqZc","executionInfo":{"status":"ok","timestamp":1610411065228,"user_tz":-60,"elapsed":1829084,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"ce4d08d8-e9e8-4e49-9a88-eb02e69f8dc5"},"source":["from torch.nn.utils.rnn import pad_sequence\r\n","\r\n","def get_loader(data, batch_size=32):\r\n","  def extract_batch(batch):\r\n","    input_ctxs = pad_sequence([ic for (ic, _), _ in batch], batch_first=True)\r\n","    input_qsts = pad_sequence([iq for (_, iq), _ in batch], batch_first=True)\r\n","    output_starts = torch.tensor([os for _, (os, _) in batch])\r\n","    output_ends = torch.tensor([oe for _, (_, oe) in batch])\r\n","    return (input_ctxs, input_qsts), (output_starts, output_ends)\r\n","  return DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=extract_batch)\r\n","\r\n","for input, output in get_loader(train_data):\r\n","  print('Input:')\r\n","  print('  > context:', input[0].shape)\r\n","  print('  > question:', input[1].shape)\r\n","  print('Output:')\r\n","  print('  > start:', output[0].shape)\r\n","  print('  > end:', output[1].shape)\r\n","  break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input:\n","  > context: torch.Size([32, 297, 96])\n","  > question: torch.Size([32, 15, 96])\n","Output:\n","  > start: torch.Size([32])\n","  > end: torch.Size([32])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LGahW6MBN5iM"},"source":["# **2. Neural Models**"]},{"cell_type":"code","metadata":{"id":"8WOGkqrXLvkB"},"source":[""],"execution_count":null,"outputs":[]}]}