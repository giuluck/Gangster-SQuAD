{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01. SpaCy.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DgAW3FJSaiZ6"},"source":["# **0. Preliminary Settings**\r\n","\r\n","At first, we need to clone the repository to get access to the code and use utility functions inside the notebook. The `src` folder is then added to the system path so that the modules can be used inside the notebook.\r\n","\r\n","Then, we use the utility functions in the `src` folder to get the ***train*** and ***validation*** splits, while we discard the ***test*** split as it will be used to evaluate the best model only."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oLOWIhI2u8rz","executionInfo":{"status":"ok","timestamp":1610213171417,"user_tz":-60,"elapsed":6521,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"bcf6c5fc-3ac0-470e-ac4b-e8271eb53578"},"source":["!git clone https://github.com/giuluck/Gangster-SQuAD\r\n","\r\n","import sys\r\n","sys.path.append('Gangster-SQuAD/src')\r\n","\r\n","from dataset import get_dataframes\r\n","train_df, val_df, _ = get_dataframes('Gangster-SQuAD/data/training_set.json')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'Gangster-SQuAD'...\n","remote: Enumerating objects: 30, done.\u001b[K\n","remote: Counting objects: 100% (30/30), done.\u001b[K\n","remote: Compressing objects: 100% (20/20), done.\u001b[K\n","remote: Total 30 (delta 8), reused 24 (delta 5), pack-reused 0\u001b[K\n","Unpacking objects: 100% (30/30), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bn13eRr3up2v","executionInfo":{"status":"ok","timestamp":1610213171424,"user_tz":-60,"elapsed":6501,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}}},"source":["## TODO: remove\r\n","train_df = train_df.iloc[:1000]\r\n","val_df = val_df.iloc[:100]"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ABWlhyiCoZ3S"},"source":["# **1. Preprocessing**\r\n","\r\n","We use **SpaCy** to tokenize both the paragraphs and the questions and then extract the *contextual embedding* of each token. We also change a little bit the default tokenizer to be able to split consecutive punctuation characters and deal with strange dashes that are in the dataset.\r\n","\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NdeeBAwEyM_d","executionInfo":{"status":"ok","timestamp":1610213173367,"user_tz":-60,"elapsed":8420,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"82c7130c-eda5-4cf2-9bca-cd811f7526dc"},"source":["import string\r\n","import spacy\r\n","from spacy.tokenizer import Tokenizer\r\n","from spacy.util import compile_infix_regex\r\n","\r\n","nlp = spacy.load('en_core_web_sm')\r\n","\r\n","split_chars = string.punctuation + '––'                            # weird dashes added\r\n","inf = list(nlp.Defaults.infixes) + [f'(?<=.)[{split_chars}](?=.)'] # always split by punctuation chars\r\n","infix_re = compile_infix_regex(tuple(inf))\r\n","\r\n","nlp.tokenizer = Tokenizer(\r\n","  nlp.vocab,\r\n","  prefix_search=nlp.tokenizer.prefix_search,\r\n","  suffix_search=nlp.tokenizer.suffix_search,\r\n","  infix_finditer=infix_re.finditer,\r\n","  token_match=nlp.tokenizer.token_match,\r\n","  rules=nlp.Defaults.tokenizer_exceptions\r\n",")\r\n","\r\n","for token in nlp(\"SpaCy is cooler than NLTK, ain't it?\"):\r\n","  print(f'{token.string:7}--> {len(token.tensor)} features')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["SpaCy  --> 96 features\n","is     --> 96 features\n","cooler --> 96 features\n","than   --> 96 features\n","NLTK   --> 96 features\n",",      --> 96 features\n","ai     --> 96 features\n","n't    --> 96 features\n","it     --> 96 features\n","?      --> 96 features\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IYOGHhaxyFPh"},"source":["### ***1.1. Tokenization***\r\n","\r\n","We process each sentences using *Spacy*.\r\n","\r\n","- **Questions** are (almost) unique in the dataset, so we process them individually and we get the matrix of *contextual embeddings* for each of them. This matrix is then appended to the respective record both in the *train* and in the *validation* dataset.\r\n","\r\n","- **Contexts**, instead, are repeated along records. Thus, as tokenizing a sentence is computationally heavy, in order to avoid to do that multiple times for the same sentences, we store the unique contexts in a set and then process them.\r\n","  * differently than for *questions*, for *contexts* we do not retrieve just the matrix of *contextual embeddings* but also the list of char *offsets* that will be used for retrieving the answer in the initial context given the two token bounaried predicted by the neural model.\r\n","  * as both of these data is heavy, we do not store them in the dataframe itself but rather in a list containing both the *embeddings* and the *offsets* of a certain *context*, so that in the dataframe we can just add a column for the respective index and avoid using memory to store the same information related to the same *contexts* multiple times.\r\n","\r\n","> Please note that this procedure should take around **15 minutes**"]},{"cell_type":"code","metadata":{"id":"1bWL-Eg4u7_k","executionInfo":{"status":"ok","timestamp":1610213173369,"user_tz":-60,"elapsed":8398,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}}},"source":["def process_sentences(sentences, extract_features=lambda doc: doc.tensor, log_rate=1000):\r\n","  output = []\r\n","  for i, sentence in enumerate(sentences):\r\n","    if log_rate > 0 and i % log_rate == 0:\r\n","      format_chars = len(str(len(sentences) - 1))\r\n","      print(f'Sentence {i:{format_chars}}/{len(sentences)}')\r\n","    doc = nlp(sentence)\r\n","    features = extract_features(doc)\r\n","    output.append(features)\r\n","  return output"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b1UWppnE-Vkn","executionInfo":{"status":"ok","timestamp":1610213186859,"user_tz":-60,"elapsed":21867,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"feea7453-5d25-4846-dc7e-57acd6c5a800"},"source":["train_df['qst_embedding'] = process_sentences(train_df['question'])"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Sentence   0/1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FVc3T6T6-hwq","executionInfo":{"status":"ok","timestamp":1610213188283,"user_tz":-60,"elapsed":23268,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"fc90246d-7cda-49f4-bc60-2b5c0bc7db91"},"source":["val_df['qst_embedding'] = process_sentences(val_df['question'])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Sentence  0/100\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a3duiQzLzSbd","executionInfo":{"status":"ok","timestamp":1610213199166,"user_tz":-60,"elapsed":34139,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"f967d5d1-d4f6-4d6b-ea61-9e4ce42df4b6"},"source":["import pandas as pd\r\n","from preprocessing import get_offsets\r\n","\r\n","contexts = pd.concat((train_df['context'], val_df['context']))\r\n","contexts = { ctx: idx for idx, ctx in enumerate(set(contexts)) }\r\n","\r\n","train_df['ctx_index'] = [contexts[ctx] for ctx in train_df['context']]\r\n","val_df['ctx_index'] = [contexts[ctx] for ctx in val_df['context']]\r\n","\r\n","contexts = process_sentences(list(contexts.keys()), extract_features=lambda ctx: {\r\n","  'ctx_embedding': ctx.tensor,\r\n","  'offsets': get_offsets([token.string for token in ctx])\r\n","})"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Sentence   0/140\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fU0tzYLgGLrp"},"source":["### ***1.2. Boundaries Computation***\r\n","\r\n","Once the contexts have been tokenized, we can compute the boundaries to identify the answer inside the context, which will be used for training and validation purposes as they will be the **outputs** of the neural model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"id":"XohneEvF3ZY0","executionInfo":{"status":"ok","timestamp":1610213199170,"user_tz":-60,"elapsed":34131,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"dde39436-25e5-4f97-d8f0-6aa281699b1d"},"source":["from preprocessing import compute_boundaries\r\n","\r\n","def add_boundaries(df):\r\n","  start_indices = []\r\n","  end_indices = []\r\n","  for _, record in df.iterrows():\r\n","    ctx = contexts[record['ctx_index']]\r\n","    start_idx, end_idx = compute_boundaries(ctx['offsets'], record['start'], len(record['answer']))\r\n","    start_indices.append(start_idx)\r\n","    end_indices.append(end_idx)\r\n","  df['start_idx'] = start_indices\r\n","  df['end_idx'] = end_indices\r\n","\r\n","add_boundaries(train_df)\r\n","add_boundaries(val_df)\r\n","\r\n","train_df[['start_idx', 'end_idx']]"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>start_idx</th>\n","      <th>end_idx</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5733be284776f41900661182</th>\n","      <td>102</td>\n","      <td>105</td>\n","    </tr>\n","    <tr>\n","      <th>5733be284776f4190066117f</th>\n","      <td>37</td>\n","      <td>42</td>\n","    </tr>\n","    <tr>\n","      <th>5733be284776f41900661180</th>\n","      <td>57</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>5733be284776f41900661181</th>\n","      <td>76</td>\n","      <td>83</td>\n","    </tr>\n","    <tr>\n","      <th>5733be284776f4190066117e</th>\n","      <td>17</td>\n","      <td>24</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>56d4f71e2ccc5a1400d833aa</th>\n","      <td>11</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>56bed4553aeaaa14008c94e5</th>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>56bed4553aeaaa14008c94e7</th>\n","      <td>31</td>\n","      <td>34</td>\n","    </tr>\n","    <tr>\n","      <th>56bed4553aeaaa14008c94e8</th>\n","      <td>55</td>\n","      <td>56</td>\n","    </tr>\n","    <tr>\n","      <th>56bfe7eaa10cfb1400551387</th>\n","      <td>10</td>\n","      <td>13</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows × 2 columns</p>\n","</div>"],"text/plain":["                          start_idx  end_idx\n","id                                          \n","5733be284776f41900661182        102      105\n","5733be284776f4190066117f         37       42\n","5733be284776f41900661180         57       60\n","5733be284776f41900661181         76       83\n","5733be284776f4190066117e         17       24\n","...                             ...      ...\n","56d4f71e2ccc5a1400d833aa         11       12\n","56bed4553aeaaa14008c94e5          1        3\n","56bed4553aeaaa14008c94e7         31       34\n","56bed4553aeaaa14008c94e8         55       56\n","56bfe7eaa10cfb1400551387         10       13\n","\n","[1000 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"hFosYV2b46Gz"},"source":["### ***1.3. Correctness Check***\r\n","\r\n","Once the dataframe is ready, we check that whether our tokenization is good enough to let us retrieve the correct answers from the text or not.\r\n","\r\n","Answers are retrived by:\r\n","1. getting the two `start` and `end` (token) boundaries that should be computed by the model\r\n","2. converting them into a `start_char` and an `end_char` pair of indices, which represent the boundaries in the original context, using the `indices` list\r\n","3. selecting the correct portion of the `context` using these two (char) boundaries and strip the obtained substring\r\n","\r\n","Some of the answers are not correct, but this is due to the fact that the answers given in the dataset contain substrings or variations of the words which are present in the text."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"763EC2EV4-F3","executionInfo":{"status":"ok","timestamp":1610213199171,"user_tz":-60,"elapsed":34122,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"cc1d8494-dac2-4091-8285-9da2a035abb4"},"source":["from preprocessing import retrieve_answer, check_correctness\r\n","\r\n","def retrieving_procedure(rec):\r\n","  ctx = contexts[rec['ctx_index']]\r\n","  return retrieve_answer(rec['start_idx'], rec['end_idx'], ctx['offsets'], rec['context'])\r\n","\r\n","check_correctness(pd.concat((train_df, val_df)), retrieving_procedure)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>answer</th>\n","      <th>normalized answer</th>\n","      <th>retrieved</th>\n","      <th>normalzed retrieved</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>56bf7e603aeaaa14008c9681</th>\n","      <td>split with Luckett and Rober</td>\n","      <td>split with luckett and rober</td>\n","      <td>split with Luckett and Roberson</td>\n","      <td>split with luckett and roberson</td>\n","    </tr>\n","    <tr>\n","      <th>56be973d3aeaaa14008c9123</th>\n","      <td>six</td>\n","      <td>six</td>\n","      <td>sixth</td>\n","      <td>sixth</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                answer  ...              normalzed retrieved\n","id                                                      ...                                 \n","56bf7e603aeaaa14008c9681  split with Luckett and Rober  ...  split with luckett and roberson\n","56be973d3aeaaa14008c9123                           six  ...                            sixth\n","\n","[2 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"l9Vh_kq0rCBj"},"source":["### ***1.4. Dataset Creation***\r\n","\r\n","We can now prepare the dataset using **Torch** utils for data managing.\r\n","\r\n","* The `Data` class extends *Torch's Dataset* and allows to get input and output data from the dataframe in a lazy way\r\n","* The `DataLoader`, then, is used to create mini-batches from the dataset and, via the custom function, to pad these mini-batches."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h10rll8EYHjp","executionInfo":{"status":"ok","timestamp":1610213692495,"user_tz":-60,"elapsed":859,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"f3403fc5-3d39-44f7-a4f8-4974176137c8"},"source":["import torch\r\n","from torch.utils.data import Dataset, DataLoader\r\n","\r\n","class Data(Dataset):\r\n","  def __init__(self, dataframe):\r\n","    self.dataframe = dataframe\r\n","        \r\n","  def __getitem__(self, index):\r\n","    rec = self.dataframe.iloc[index]\r\n","    ctx = contexts[rec['ctx_index']]\r\n","    input_ctx = torch.tensor(ctx['ctx_embedding'])\r\n","    input_qst = torch.tensor(rec['qst_embedding'])\r\n","    output_start = torch.tensor(rec['start_idx'])\r\n","    output_end = torch.tensor(rec['end_idx'])\r\n","    return (input_ctx, input_qst), (output_start, output_end)\r\n","  \r\n","  def __len__(self):\r\n","    return len(self.dataframe)\r\n","\r\n","train_data = Data(train_df)\r\n","val_data = Data(val_df)\r\n","\r\n","input, output = train_data[0]\r\n","print('Input:')\r\n","print('  > context:', input[0].shape)\r\n","print('  > question:', input[1].shape)\r\n","print('Output:')\r\n","print('  > start:', output[0].shape)\r\n","print('  > end:', output[1].shape)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Input:\n","  > context: torch.Size([142, 96])\n","  > question: torch.Size([14, 96])\n","Output:\n","  > start: torch.Size([])\n","  > end: torch.Size([])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4gUGJ3-DfqZc","executionInfo":{"status":"ok","timestamp":1610213693725,"user_tz":-60,"elapsed":579,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"0e891551-e937-43b3-9a3e-fee68d9a1d03"},"source":["from torch.nn.utils.rnn import pad_sequence\r\n","\r\n","def get_loader(data, batch_size=32):\r\n","  def extract_batch(batch):\r\n","    input_ctxs = pad_sequence([ic for (ic, _), _ in batch], batch_first=True)\r\n","    input_qsts = pad_sequence([iq for (_, iq), _ in batch], batch_first=True)\r\n","    output_starts = torch.tensor([os for _, (os, _) in batch])\r\n","    output_ends = torch.tensor([oe for _, (_, oe) in batch])\r\n","    return (input_ctxs, input_qsts), (output_starts, output_ends)\r\n","  return DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=extract_batch)\r\n","\r\n","for input, output in get_loader(train_data):\r\n","  print('Input:')\r\n","  print('  > context:', input[0].shape)\r\n","  print('  > question:', input[1].shape)\r\n","  print('Output:')\r\n","  print('  > start:', output[0].shape)\r\n","  print('  > end:', output[1].shape)\r\n","  break"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Input:\n","  > context: torch.Size([32, 404, 96])\n","  > question: torch.Size([32, 19, 96])\n","Output:\n","  > start: torch.Size([32])\n","  > end: torch.Size([32])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LGahW6MBN5iM"},"source":["# **2. Neural Model**"]},{"cell_type":"code","metadata":{"id":"8WOGkqrXLvkB"},"source":[""],"execution_count":null,"outputs":[]}]}