{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "SpaCy Models.ipynb",
   "private_outputs": true,
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3tOEic89H89"
   },
   "source": [
    "> **SPACY**\r\n",
    ">\r\n",
    "> ---\r\n",
    ">\r\n",
    "> In this notebook we develop a neural model leveraging **SpaCy**. We preprocess `paragraphs` and `questions` using the english language model in order to tokenize the sentences and retrieve their contextual embeddings. Finally, we use these embeddings as inputs for the model to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgAW3FJSaiZ6"
   },
   "source": [
    "# **0. Preliminary Settings**\r\n",
    "\r\n",
    "At first, we need to clone the repository to get access to the code and use utility functions inside the notebook. The `src` folder is then added to the system path so that the modules can be used inside the notebook.\r\n",
    "\r\n",
    "Then, we use the utility functions in the `src` folder to get the ***train*** and ***validation*** splits, while we discard the ***test*** split as it will be used to evaluate the best model only."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oLOWIhI2u8rz"
   },
   "source": [
    "!git clone https://github.com/giuluck/Gangster-SQuAD\r\n",
    "\r\n",
    "import sys\r\n",
    "sys.path.append('Gangster-SQuAD/src')\r\n",
    "\r\n",
    "from dataset import get_dataframes\r\n",
    "train_df, val_df, _ = get_dataframes('Gangster-SQuAD/data/training_set.json')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bn13eRr3up2v"
   },
   "source": [
    "## TODO: remove\r\n",
    "train_df = train_df.iloc[:1000]\r\n",
    "val_df = val_df.iloc[:100]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABWlhyiCoZ3S"
   },
   "source": [
    "# **1. Preprocessing**\r\n",
    "\r\n",
    "We use **SpaCy** to tokenize both the paragraphs and the questions and then extract the *contextual embedding* of each token. We also change a little bit the default tokenizer to be able to split consecutive punctuation characters and deal with strange dashes that are in the dataset.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sQ1XitX-12z9"
   },
   "source": [
    "%%capture\r\n",
    "!python -m spacy download en_core_web_md\r\n",
    "!python -m spacy link en_core_web_md en_core_web_md"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NdeeBAwEyM_d"
   },
   "source": [
    "import string\r\n",
    "import spacy\r\n",
    "from spacy.tokenizer import Tokenizer\r\n",
    "from spacy.util import compile_infix_regex\r\n",
    "\r\n",
    "nlp = spacy.load('en_core_web_md')\r\n",
    "\r\n",
    "split_chars = string.punctuation + '––'                            # weird dashes added\r\n",
    "inf = list(nlp.Defaults.infixes) + [f'(?<=.)[{split_chars}](?=.)'] # always split by punctuation chars\r\n",
    "infix_re = compile_infix_regex(tuple(inf))\r\n",
    "\r\n",
    "nlp.tokenizer = Tokenizer(\r\n",
    "  nlp.vocab,\r\n",
    "  prefix_search=nlp.tokenizer.prefix_search,\r\n",
    "  suffix_search=nlp.tokenizer.suffix_search,\r\n",
    "  infix_finditer=infix_re.finditer,\r\n",
    "  token_match=nlp.tokenizer.token_match,\r\n",
    "  rules=nlp.Defaults.tokenizer_exceptions\r\n",
    ")\r\n",
    "\r\n",
    "for token in nlp(\"SpaCy is cooler than NLTK, ain't it?\"):\r\n",
    "  print(f'{token.string:7}--> {len(token.tensor)} features')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYOGHhaxyFPh"
   },
   "source": [
    "### ***1.1. Tokenization***\r\n",
    "\r\n",
    "We process each sentences using *Spacy*.\r\n",
    "\r\n",
    "- **Questions** are (almost) unique in the dataset, so we process them individually and we get the matrix of *contextual embeddings* for each of them. This matrix is then appended to the respective record both in the *train* and in the *validation* dataset.\r\n",
    "\r\n",
    "- **Contexts**, instead, are repeated along records. Thus, as tokenizing a sentence is computationally heavy, in order to avoid to do that multiple times for the same sentences, we store the unique contexts in a set and then process them.\r\n",
    "  * differently than for *questions*, for *contexts* we do not retrieve just the matrix of *contextual embeddings* but also the list of char *offsets* that will be used for retrieving the answer in the initial context given the two token bounaried predicted by the neural model.\r\n",
    "  * as both of these data is heavy, we do not store them in the dataframe itself but rather in a list containing both the *embeddings* and the *offsets* of a certain *context*, so that in the dataframe we can just add a column for the respective index and avoid using memory to store the same information related to the same *contexts* multiple times.\r\n",
    "\r\n",
    "> Please note that this procedure should take around **20 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1bWL-Eg4u7_k"
   },
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "def process_sentences(sentences, extract_features, log_rate=1000):\r\n",
    "  output = []\r\n",
    "  for i, sentence in enumerate(sentences):\r\n",
    "    if log_rate > 0 and i % log_rate == 0:\r\n",
    "      format_chars = len(str(len(sentences) - 1))\r\n",
    "      print(f'Sentence {i:{format_chars}}/{len(sentences)}')\r\n",
    "    doc = nlp(sentence)\r\n",
    "    features = extract_features(doc)\r\n",
    "    output.append(features)\r\n",
    "  return output\r\n",
    "\r\n",
    "def extract_embeddings(doc):\r\n",
    "  return np.array([np.concatenate((tok.vector, tok.tensor)) for tok in doc])\r\n",
    "\r\n",
    "extract_embeddings(nlp(\"SpaCy is cooler than NLTK, ain't it?\")).shape"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b1UWppnE-Vkn"
   },
   "source": [
    "train_df['qst_embedding'] = process_sentences(train_df['question'], extract_features=extract_embeddings)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FVc3T6T6-hwq"
   },
   "source": [
    "val_df['qst_embedding'] = process_sentences(val_df['question'], extract_features=extract_embeddings)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "a3duiQzLzSbd"
   },
   "source": [
    "import pandas as pd\r\n",
    "from preprocessing import get_offsets\r\n",
    "\r\n",
    "contexts = pd.concat((train_df['context'], val_df['context']))\r\n",
    "contexts = { ctx: idx for idx, ctx in enumerate(set(contexts)) }\r\n",
    "\r\n",
    "train_df['ctx_index'] = [contexts[ctx] for ctx in train_df['context']]\r\n",
    "val_df['ctx_index'] = [contexts[ctx] for ctx in val_df['context']]\r\n",
    "\r\n",
    "contexts = process_sentences(list(contexts.keys()), extract_features=lambda ctx: {\r\n",
    "  'ctx_embedding': extract_embeddings(ctx),\r\n",
    "  'offsets': get_offsets([token.string for token in ctx])\r\n",
    "})"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fU0tzYLgGLrp"
   },
   "source": [
    "### ***1.2. Boundaries Computation***\r\n",
    "\r\n",
    "Once the contexts have been tokenized, we can compute the boundaries to identify the answer inside the context, which will be used for training and validation purposes as they will be the **outputs** of the neural model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XohneEvF3ZY0"
   },
   "source": [
    "from preprocessing import compute_boundaries\r\n",
    "\r\n",
    "def add_boundaries(df):\r\n",
    "  start_indices = []\r\n",
    "  end_indices = []\r\n",
    "  for _, record in df.iterrows():\r\n",
    "    ctx = contexts[record['ctx_index']]\r\n",
    "    start_idx, end_idx = compute_boundaries(ctx['offsets'], record['start'], len(record['answer']))\r\n",
    "    start_indices.append(start_idx)\r\n",
    "    end_indices.append(end_idx)\r\n",
    "  df['start_idx'] = start_indices\r\n",
    "  df['end_idx'] = end_indices\r\n",
    "\r\n",
    "add_boundaries(train_df)\r\n",
    "add_boundaries(val_df)\r\n",
    "\r\n",
    "train_df[['start_idx', 'end_idx']]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFosYV2b46Gz"
   },
   "source": [
    "### ***1.3. Correctness Check***\r\n",
    "\r\n",
    "Once the dataframe is ready, we check that whether our tokenization is good enough to let us retrieve the correct answers from the text or not.\r\n",
    "\r\n",
    "Answers are retrived by:\r\n",
    "1. getting the two `start` and `end` (token) boundaries that should be computed by the model\r\n",
    "2. converting them into a `start_char` and an `end_char` pair of indices, which represent the boundaries in the original context, using the `indices` list\r\n",
    "3. selecting the correct portion of the `context` using these two (char) boundaries and strip the obtained substring\r\n",
    "\r\n",
    "Some of the answers are not correct, but this is due to the fact that the answers given in the dataset contain substrings or variations of the words which are present in the text."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "763EC2EV4-F3"
   },
   "source": [
    "from preprocessing import retrieve_answer\r\n",
    "from dataframe import check_correctness\r\n",
    "\r\n",
    "def retrieving_procedure(rec):\r\n",
    "  ctx = contexts[rec['ctx_index']]\r\n",
    "  return retrieve_answer(rec['start_idx'], rec['end_idx'], ctx['offsets'], rec['context'])\r\n",
    "\r\n",
    "check_correctness(pd.concat((train_df, val_df)), retrieving_procedure)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGahW6MBN5iM"
   },
   "source": [
    "# **2. Neural Models**\r\n",
    "\r\n",
    "In this section, we will prepare the dataset using **Torch** utils for data managing, then we will implement different neural models and evaluate their scores on validation data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aeHP2pouYeTt"
   },
   "source": [
    "%%capture\r\n",
    "!pip install wandb pytorch-lightning"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qgF-s7v2sUuI"
   },
   "source": [
    "import torch\r\n",
    "import pytorch_lightning as pl\r\n",
    "\r\n",
    "from torch import nn\r\n",
    "from torch.utils.data import Dataset, DataLoader\r\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\r\n",
    "from pytorch_lightning.callbacks import EarlyStopping"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgWVuW_VpcHz"
   },
   "source": [
    "### ***2.1. LSTM + Categorical Heads***\r\n",
    "\r\n",
    "In this model, context and questions are concatenated (an *empty* separation token is place between them) and processed via a **Bidirectional LSTM**. The outputs of this layer is then processed via two fully-connected layers to obtain the *logits* for both the start and the end tokens, which will be used to train the model via **Categorical Cross-Entropy**. An **alpha** parameter could be set to give more importance to the *start loss* with respect to the *end loss*, or vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7nM_CWbrrZTW"
   },
   "source": [
    "class SquadDatasetLSTM(Dataset):\r\n",
    "  def __init__(self, dataframe):\r\n",
    "    ctx_lengths = dataframe['ctx_index'].map(lambda idx: len(contexts[idx]['ctx_embedding']))\r\n",
    "    qst_lengths = dataframe['qst_embedding'].map(lambda emb: len(emb))\r\n",
    "    self.max_len = max([cl + ql for cl, ql in zip(ctx_lengths, qst_lengths)]) + 1 # +1 for the sep token\r\n",
    "    self.dataframe = dataframe\r\n",
    "        \r\n",
    "  def __getitem__(self, index):\r\n",
    "    rec = self.dataframe.iloc[index]\r\n",
    "    ctx = contexts[rec['ctx_index']]\r\n",
    "    input_ctx = torch.tensor(ctx['ctx_embedding'])\r\n",
    "    input_qst = torch.tensor(rec['qst_embedding'])\r\n",
    "    input_sep = torch.zeros((1, input_ctx.shape[1]))\r\n",
    "    input_len = len(input_ctx) + len(input_qst) + 1\r\n",
    "    input_pad = torch.zeros((self.max_len - input_len, input_ctx.shape[1]))\r\n",
    "    input = torch.cat((input_ctx, input_sep, input_qst, input_pad))\r\n",
    "    output = torch.tensor([rec['start_idx'], rec['end_idx']])\r\n",
    "    return (input, input_len), output\r\n",
    "  \r\n",
    "  def __len__(self):\r\n",
    "    return len(self.dataframe)\r\n",
    "\r\n",
    "train_data = SquadDatasetLSTM(train_df)\r\n",
    "val_data = SquadDatasetLSTM(val_df)\r\n",
    "\r\n",
    "(input, _), output = train_data[0]\r\n",
    "print('Input:', input.shape)\r\n",
    "print('Output:', output.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JVbzMP27rZIq"
   },
   "source": [
    "def collate_fn(samples):\r\n",
    "  inputs = torch.stack([input for (input, _), _ in samples])\r\n",
    "  lengths = torch.tensor([length for (_, length), _ in samples])\r\n",
    "  outputs = torch.stack([output for _, output in samples])\r\n",
    "  return pack_padded_sequence(inputs, lengths, batch_first=True, enforce_sorted=False), outputs\r\n",
    "\r\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate_fn)\r\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zsjSZILJp3Js"
   },
   "source": [
    "class SpacyCategoricalLSTM(pl.LightningModule):\r\n",
    "    def __init__(self, embedding_dim=396, hidden_dim=128, alpha=0.5):\r\n",
    "        super().__init__()\r\n",
    "        self.alpha = alpha\r\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\r\n",
    "        self.start_fc = nn.Linear(2 * hidden_dim, 1)\r\n",
    "        self.end_fc = nn.Linear(2 * hidden_dim, 1)\r\n",
    "        self.softmax = nn.Softmax(dim=1)\r\n",
    "        self.criterion = nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "    def _logits(self, x):\r\n",
    "        x, _ = self.rnn(x)\r\n",
    "        x, _ = pad_packed_sequence(x, batch_first=True)\r\n",
    "        start = self.start_fc(x).squeeze(dim=2)\r\n",
    "        end = self.end_fc(x).squeeze(dim=2)\r\n",
    "        return start, end\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        start, end = self._logits(x)\r\n",
    "        prob_start = self.softmax(start)\r\n",
    "        prob_end = self.softmax(end)\r\n",
    "        return prob_start, prob_end\r\n",
    "    \r\n",
    "    def _step(self, batch, batch_idx):\r\n",
    "        x, y = batch\r\n",
    "        pred_start, pred_end = self._logits(x)\r\n",
    "        loss_start = self.criterion(pred_start, y[:, 0])\r\n",
    "        loss_end = self.criterion(pred_end, y[:, 1])\r\n",
    "        return loss_start, loss_end\r\n",
    "\r\n",
    "    def training_step(self, batch, batch_idx):\r\n",
    "        loss_start, loss_end = self._step(batch, batch_idx)\r\n",
    "        loss = self.alpha * loss_start + (1 - self.alpha) * loss_end\r\n",
    "        # On training must be specified on epoch and not on step\r\n",
    "        self.log('train/loss_start', loss_start, on_epoch=True, on_step=False, \r\n",
    "             prog_bar=True)\r\n",
    "        self.log('train/loss_end', loss_end, on_epoch=True, on_step=False, \r\n",
    "             prog_bar=True)\r\n",
    "        return loss\r\n",
    "\r\n",
    "    def validation_step(self, batch, batch_idx):\r\n",
    "        loss_start, loss_end = self._step(batch, batch_idx)\r\n",
    "        loss = self.alpha * loss_start + (1 - self.alpha) * loss_end\r\n",
    "        # On validation automatically logs only on epochs\r\n",
    "        self.log('val/loss_start', loss_start, prog_bar=True)\r\n",
    "        self.log('val/loss_end', loss_end, prog_bar=True)\r\n",
    "        self.log('val/loss', loss, prog_bar=True)\r\n",
    "        return loss\r\n",
    "\r\n",
    "    def configure_optimizers(self):\r\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\r\n",
    "        return optimizer\r\n",
    "\r\n",
    "spacy_categorical_LSTM = SpacyCategoricalLSTM()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m5ZUqxuLFC7J"
   },
   "source": [
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# Access to W&B\n",
    "wandb.login()\n",
    "\n",
    "project_name = \"SQuAD\"\n",
    "# Automatically assigned if None, duplicates are automatically handled\n",
    "wandb_run_name = None\n",
    "wandb_entity = \"lomb\"\n",
    "\n",
    "# Create the logger\n",
    "wandb_logger = WandbLogger(project=\"SQuAD\", name=wandb_run_name)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mWN8r1-vqiEu"
   },
   "source": [
    "trainer = pl.Trainer(\r\n",
    "  max_epochs=20, gpus=-1, logger=wandb_logger, checkpoint_callback=False,\r\n",
    "  callbacks=[EarlyStopping(monitor='val/loss', mode='min', patience=3)]\r\n",
    ")\r\n",
    "\r\n",
    "trainer.fit(spacy_categorical_LSTM, train_loader, val_loader)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wOoq72PjFJHz"
   },
   "source": [
    "# Run this to stop the synchronization with the previous run\n",
    "wandb.finish()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wxxvK2HsqlDp"
   },
   "source": [
    "starts, ends = [], []\r\n",
    "for input, output in val_loader:\r\n",
    "  s, e = spacy_categorical_LSTM(input)\r\n",
    "  _, s = s.max(dim=1)\r\n",
    "  _, e = e.max(dim=1)\r\n",
    "  starts.append(s.numpy())\r\n",
    "  ends.append(e.numpy())\r\n",
    "\r\n",
    "val_df['pred_start'] = [s for ss in starts for s in ss]\r\n",
    "val_df['pred_end'] = [e for ee in ends for e in ee]\r\n",
    "val_df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t-M2J27-qlg-"
   },
   "source": [
    "def retrieving_procedure(rec):\r\n",
    "  ctx = contexts[rec['ctx_index']]\r\n",
    "  max_len = len(ctx['offsets']) - 1\r\n",
    "  pred_start = min(rec['pred_start'], max_len)\r\n",
    "  pred_end = min(rec['pred_end'], max_len)\r\n",
    "  return retrieve_answer(pred_start, pred_end, ctx['offsets'], rec['context'])\r\n",
    "\r\n",
    "wrong_answers = check_correctness(val_df, retrieving_procedure)\r\n",
    "wrong_answers"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MU-efujuqmRw"
   },
   "source": [
    "print('Validation Score:', 1 - len(wrong_answers) / len(val_df))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEgZf_d4ql_S"
   },
   "source": [
    "### ***2.2. Transformer + Categorical Heads***\r\n",
    "\r\n",
    "In this model, context and questions are passed as *target* and *source* of a **Transformer** model, respectively. As before, the outputs of this layer is processed via two fully-connected layers to obtain the *logits* for both the start and the end tokens, which will be used to train the model via **Categorical Cross-Entropy**, with the same **alpha** parameter."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d8bh4lfIhXy_"
   },
   "source": [
    "class SquadDatasetTransformer(Dataset):\r\n",
    "  def __init__(self, dataframe):\r\n",
    "    self.dataframe = dataframe\r\n",
    "    self.max_ctx = max([len(contexts[ctx]['ctx_embedding']) for ctx in dataframe['ctx_index']])\r\n",
    "    self.max_qst = max([len(qst) for qst in dataframe['qst_embedding']])\r\n",
    "        \r\n",
    "  def __getitem__(self, index):\r\n",
    "    # data\r\n",
    "    rec = self.dataframe.iloc[index]\r\n",
    "    qst_emb = rec['qst_embedding']\r\n",
    "    ctx_emb = contexts[rec['ctx_index']]['ctx_embedding']\r\n",
    "    # context\r\n",
    "    ctx_padding = torch.zeros((self.max_ctx - len(ctx_emb), ctx_emb.shape[1]))\r\n",
    "    ctx_tokens = torch.cat((torch.tensor(ctx_emb), ctx_padding))\r\n",
    "    ctx_mask = torch.cat((torch.zeros(len(ctx_emb)), torch.ones(self.max_ctx - len(ctx_emb)))).type(torch.BoolTensor)\r\n",
    "    # question\r\n",
    "    qst_padding = torch.zeros((self.max_qst - len(qst_emb), qst_emb.shape[1]))\r\n",
    "    qst_tokens = torch.cat((torch.tensor(qst_emb), qst_padding))\r\n",
    "    qst_mask = torch.cat((torch.zeros(len(qst_emb)), torch.ones(self.max_qst - len(qst_emb)))).type(torch.BoolTensor)\r\n",
    "    # outputs\r\n",
    "    output = torch.tensor([rec['start_idx'], rec['end_idx']])\r\n",
    "    return (ctx_tokens, ctx_mask), (qst_tokens, qst_mask), output\r\n",
    "  \r\n",
    "  def __len__(self):\r\n",
    "    return len(self.dataframe)\r\n",
    "\r\n",
    "train_data = SquadDatasetTransformer(train_df)\r\n",
    "val_data = SquadDatasetTransformer(val_df)\r\n",
    "\r\n",
    "(ctx_tokens, ctx_mask), (qst_tokens, qst_mask), output = train_data[0]\r\n",
    "print('Input Context:')\r\n",
    "print('  > tokens:', ctx_tokens.shape)\r\n",
    "print('  > mask:', ctx_mask.shape)\r\n",
    "print('Input Question:')\r\n",
    "print('  > tokens:', qst_tokens.shape)\r\n",
    "print('  > mask:', qst_mask.shape)\r\n",
    "print('Output:', output.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Rm0HP9rJA2L7"
   },
   "source": [
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\r\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\r\n",
    "\r\n",
    "for (ctx_tokens, ctx_masks), (qst_tokens, qst_masks), output in train_loader:\r\n",
    "  print('Input Context:')\r\n",
    "  print('  > tokens:', ctx_tokens.shape)\r\n",
    "  print('  > mask:', ctx_masks.shape)\r\n",
    "  print('Input Question:')\r\n",
    "  print('  > tokens:', qst_tokens.shape)\r\n",
    "  print('  > mask:', qst_masks.shape)\r\n",
    "  print('Output:', output.shape)\r\n",
    "  break"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8WOGkqrXLvkB"
   },
   "source": [
    "class SpacyCategoricalTransformer(pl.LightningModule):\r\n",
    "    def __init__(self, embedding_dim=396, alpha=0.5):\r\n",
    "        super().__init__()\r\n",
    "        self.alpha = alpha\r\n",
    "        self.transformer = nn.Transformer(embedding_dim, nhead=6)\r\n",
    "        self.start_fc = nn.Linear(embedding_dim, 1)\r\n",
    "        self.end_fc = nn.Linear(embedding_dim, 1)\r\n",
    "        self.softmax = nn.Softmax(dim=1)\r\n",
    "        self.criterion = nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "    def _logits(self, ctx_tokens, qst_tokens, ctx_masks, qst_masks):\r\n",
    "        ctx_tokens = ctx_tokens.transpose(0, 1)\r\n",
    "        qst_tokens = qst_tokens.transpose(0, 1)\r\n",
    "        x = self.transformer(src=qst_tokens, tgt=ctx_tokens, src_key_padding_mask=qst_masks, tgt_key_padding_mask=ctx_masks)\r\n",
    "        x = x.transpose(0, 1)\r\n",
    "        start = self.start_fc(x).squeeze(dim=2)\r\n",
    "        end = self.end_fc(x).squeeze(dim=2)\r\n",
    "        return start, end\r\n",
    "\r\n",
    "    def forward(self, ctx_tokens, qst_tokens, ctx_masks, qst_masks):\r\n",
    "        start, end = self._logits(ctx_tokens, qst_tokens, ctx_masks, qst_masks)\r\n",
    "        prob_start = self.softmax(start)\r\n",
    "        prob_end = self.softmax(end)\r\n",
    "        return prob_start, prob_end\r\n",
    "    \r\n",
    "    def _step(self, batch, batch_idx):\r\n",
    "        (ctx_tokens, ctx_masks), (qst_tokens, qst_masks), out = batch\r\n",
    "        pred_start, pred_end = self._logits(ctx_tokens, qst_tokens, ctx_masks, qst_masks)\r\n",
    "        loss_start = self.criterion(pred_start, out[:, 0])\r\n",
    "        loss_end = self.criterion(pred_end, out[:, 1])\r\n",
    "        return loss_start, loss_end\r\n",
    "\r\n",
    "    def training_step(self, batch, batch_idx):\r\n",
    "        loss_start, loss_end = self._step(batch, batch_idx)\r\n",
    "        loss = self.alpha * loss_start + (1 - self.alpha) * loss_end\r\n",
    "        self.log('train/loss_start', loss_start, on_epoch=True, on_step=False, \r\n",
    "             prog_bar=True)\r\n",
    "        self.log('train/loss_end', loss_end, on_epoch=True, on_step=False, \r\n",
    "             prog_bar=True)\r\n",
    "        return loss\r\n",
    "\r\n",
    "    def validation_step(self, batch, batch_idx):\r\n",
    "        loss_start, loss_end = self._step(batch, batch_idx)\r\n",
    "        loss = self.alpha * loss_start + (1 - self.alpha) * loss_end\r\n",
    "        self.log('val/loss_start', loss_start, prog_bar=True)\r\n",
    "        self.log('val/loss_end', loss_end, prog_bar=True)\r\n",
    "        self.log('val/loss', loss, prog_bar=True)\r\n",
    "        return loss\r\n",
    "\r\n",
    "    def configure_optimizers(self):\r\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\r\n",
    "        return optimizer\r\n",
    "\r\n",
    "spacy_categorical_transformer = SpacyCategoricalTransformer()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2XPw43nfFWHd"
   },
   "source": [
    "run_name = None\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"SQuAD\", entity=wandb_entity, name=wandb_run_name)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PMF7nKA7XPZJ"
   },
   "source": [
    "trainer = pl.Trainer(\r\n",
    "  max_epochs=20, gpus=-1, logger=wandb_logger, checkpoint_callback=False,\r\n",
    "  callbacks=[EarlyStopping(monitor='val/loss', mode='min', patience=3)]\r\n",
    ")\r\n",
    "\r\n",
    "trainer.fit(spacy_categorical_transformer, train_loader, val_loader)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VQo5OBcxFZPX"
   },
   "source": [
    "# Run this to stop the synchronization with the previous run\n",
    "wandb.finish()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1ylQHn8pptTn"
   },
   "source": [
    "starts, ends = [], []\r\n",
    "for (ctx_tokens, ctx_masks), (qst_tokens, qst_masks), output in val_loader:\r\n",
    "  s, e = spacy_categorical_transformer(ctx_tokens, qst_tokens, ctx_masks, qst_masks)\r\n",
    "  _, s = s.max(dim=1)\r\n",
    "  _, e = e.max(dim=1)\r\n",
    "  starts.append(s.numpy())\r\n",
    "  ends.append(e.numpy())\r\n",
    "\r\n",
    "val_df['pred_start'] = [s for ss in starts for s in ss]\r\n",
    "val_df['pred_end'] = [e for ee in ends for e in ee]\r\n",
    "val_df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X6EX7MxQzvlt"
   },
   "source": [
    "def retrieving_procedure(rec):\r\n",
    "  ctx = contexts[rec['ctx_index']]\r\n",
    "  max_len = len(ctx['offsets']) - 1\r\n",
    "  pred_start = min(rec['pred_start'], max_len)\r\n",
    "  pred_end = min(rec['pred_end'], max_len)\r\n",
    "  return retrieve_answer(pred_start, pred_end, ctx['offsets'], rec['context'])\r\n",
    "\r\n",
    "wrong_answers = check_correctness(val_df, retrieving_procedure)\r\n",
    "wrong_answers"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L35UL0sC0F_I"
   },
   "source": [
    "print('Validation Score:', 1 - len(wrong_answers) / len(val_df))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EyP5Ayn6smFL"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
