{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02. BERT Base Uncased.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"03c9a517458f4b00992aeaf481a262cb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3258f8b8847f4f76b346887e8111b6fb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_673b2d7bf983489991ea7314004b43f8","IPY_MODEL_41f4094c362241f69358fc643e04f0dc"]}},"3258f8b8847f4f76b346887e8111b6fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"673b2d7bf983489991ea7314004b43f8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b1dd821fb66e414ea3f3a717a8172b92","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f3d7230952b543e28d87f4fb233fc5de"}},"41f4094c362241f69358fc643e04f0dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5ca300404cc44eb9808dc03fd7558ed6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 308kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1554bef4494c4dc981375575795ee848"}},"b1dd821fb66e414ea3f3a717a8172b92":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f3d7230952b543e28d87f4fb233fc5de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5ca300404cc44eb9808dc03fd7558ed6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1554bef4494c4dc981375575795ee848":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6a298f48f28149368d6481da391852b0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c6c5d24eb25f47bea93508a967eb6e23","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b282027a522542238bbcd72ee37bc9f0","IPY_MODEL_75330b0d1d2b470eb92bb0b1920235c6"]}},"c6c5d24eb25f47bea93508a967eb6e23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b282027a522542238bbcd72ee37bc9f0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_51044f3a924e47a8a17fa4b3afd65d4c","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bdcc30a368bd45c1802903415b31f094"}},"75330b0d1d2b470eb92bb0b1920235c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8168866578ea478eb3bf9df147db9722","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:00&lt;00:00, 968B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ef6ae12e1fb14832a9100a63433624ca"}},"51044f3a924e47a8a17fa4b3afd65d4c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bdcc30a368bd45c1802903415b31f094":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8168866578ea478eb3bf9df147db9722":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ef6ae12e1fb14832a9100a63433624ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e38f8d78c9b04bcf9686a006f79671f0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f7284d615851498592deaeeba6cc940c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4a54e4277a4e48b28e1cf38443acc3be","IPY_MODEL_82a38472f76445f0bc197156c357ea75"]}},"f7284d615851498592deaeeba6cc940c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4a54e4277a4e48b28e1cf38443acc3be":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4b684a865cf84b5793ffd1cbb1c6aefd","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_89992d0dd12a43a2bc15bbb7b8361aa3"}},"82a38472f76445f0bc197156c357ea75":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5ee929e4eda5473bb704ad9c699af986","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [00:06&lt;00:00, 64.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_14a32296333b44e3b039e70a7b8ed9da"}},"4b684a865cf84b5793ffd1cbb1c6aefd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"89992d0dd12a43a2bc15bbb7b8361aa3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5ee929e4eda5473bb704ad9c699af986":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"14a32296333b44e3b039e70a7b8ed9da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"b5_hByr9-Asu"},"source":["> **BERT BASE UNCASED**\r\n",">\r\n","> ---\r\n",">\r\n","> In this notebook we develop a neural model starting from a pre-trained **BERT** model. We tokenize `paragraphs` and `questions` using the default tokenizer, then use the pretrained model as first layer for the neural network."]},{"cell_type":"markdown","metadata":{"id":"DgAW3FJSaiZ6"},"source":["# **0. Preliminary Settings**\r\n","\r\n","At first, we need to clone the repository to get access to the code and use utility functions inside the notebook. The `src` folder is then added to the system path so that the modules can be used inside the notebook.\r\n","\r\n","Then, we use the utility functions in the `src` folder to get the ***train*** and ***validation*** splits, while we discard the ***test*** split as it will be used to evaluate the best model only."]},{"cell_type":"code","metadata":{"id":"p_fTKyb3S8_7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610214938129,"user_tz":-60,"elapsed":14522,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"75bfe844-c60e-4ab9-a5f8-73b02f1c00f9"},"source":["!git clone https://github.com/giuluck/Gangster-SQuAD\r\n","\r\n","import sys\r\n","sys.path.append('Gangster-SQuAD/src')\r\n","\r\n","from dataset import get_dataframes\r\n","train_df, val_df, _ = get_dataframes('Gangster-SQuAD/data/training_set.json')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'Gangster-SQuAD'...\n","remote: Enumerating objects: 30, done.\u001b[K\n","remote: Counting objects: 100% (30/30), done.\u001b[K\n","remote: Compressing objects: 100% (20/20), done.\u001b[K\n","remote: Total 30 (delta 8), reused 24 (delta 5), pack-reused 0\u001b[K\n","Unpacking objects: 100% (30/30), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q9hObEuqQzAS"},"source":["## TODO: remove\r\n","train_df = train_df.iloc[:1000]\r\n","val_df = val_df.iloc[:100]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"49Oyo_iYbUXC"},"source":["# **1. Preprocessing**\r\n","\r\n","We use **HuggingFace** to start from a pretrained **BERT** model with its own vocabulary and tokenizer."]},{"cell_type":"code","metadata":{"id":"uwjHUNHCBqwM"},"source":["%%capture\r\n","!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E9fGH4MhB-u-"},"source":["### ***1.1. Tokenization***\r\n","\r\n","**HuggingFace** provides a pretrained *BertTokenizer*, which is quite slow, and a faster *BertWordPieceTokenizer*. In order to exploit both of them, we initially load the pretrained tokenizer, store its data, and use that data to build the faster tokenizer.\r\n","\r\n","Once the tokenizer has been built, we use it to process every record in the dataframe in order to build the dataset used for training and testing purposes. This dataset will be composed by:\r\n","- a **list of tokens** structured in this way $$[\\mbox{CLS}, \\mbox{ctx_tok}_0, ..., \\mbox{ctx_tok}_i, ..., \\mbox{ctx_tok}_n, \\mbox{SEP}, \\mbox{qst_tok}_0, ..., \\mbox{qst_tok}_j, ..., \\mbox{qst_tok}_m, \\mbox{SEP}]$$ which will be used as input for the *BERT* model, with the respective lists of **type ids** (*0* for the context, *1* for the answer)\r\n","- a **start** and an **end** integer value representing the indices of the boundary tokens that identify the answer in the text, which will be used as outputs for the *BERT* model\r\n","- the **original context** and a **list of indices** representing the offsets, expressed in number of *chars* and not in number of *tokens*, which will be used to retrieve the original part of text in the context given the two outputs"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["03c9a517458f4b00992aeaf481a262cb","3258f8b8847f4f76b346887e8111b6fb","673b2d7bf983489991ea7314004b43f8","41f4094c362241f69358fc643e04f0dc","b1dd821fb66e414ea3f3a717a8172b92","f3d7230952b543e28d87f4fb233fc5de","5ca300404cc44eb9808dc03fd7558ed6","1554bef4494c4dc981375575795ee848"]},"id":"7pPh7k4zCH0T","executionInfo":{"status":"ok","timestamp":1610214953037,"user_tz":-60,"elapsed":29346,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"457eed39-49a1-4fe8-cd1a-de980d7700f7"},"source":["from transformers import BertTokenizer\r\n","from tokenizers import BertWordPieceTokenizer\r\n","\r\n","pretrained_model = 'bert-base-uncased'\r\n","\r\n","BertTokenizer.from_pretrained(pretrained_model).save_pretrained('slow_tokenizer/')\r\n","tokenizer = BertWordPieceTokenizer('slow_tokenizer/vocab.txt', lowercase=True)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"03c9a517458f4b00992aeaf481a262cb","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OZqIABrQtTNk","executionInfo":{"status":"ok","timestamp":1610214953040,"user_tz":-60,"elapsed":29327,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"cf145b9b-fb04-463f-ca85-a511f921dbfa"},"source":["special_tokens = { token: id for token, id in zip(['[CLS]', '[PAD]', '[SEP]'], tokenizer.encode('[PAD]').ids) }\r\n","special_tokens"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'[CLS]': 101, '[PAD]': 0, '[SEP]': 102}"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":646},"id":"doW5C2l1CgYf","executionInfo":{"status":"ok","timestamp":1610214999535,"user_tz":-60,"elapsed":75793,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"cc632c0b-e780-455b-feec-b847fbe32615"},"source":["import pandas as pd\r\n","from preprocessing import compute_boundaries\r\n","\r\n","def process_dataframe(df):\r\n","  def process_record(record):\r\n","    # both context and question gets tokenized\r\n","    tokens = tokenizer.encode(record['context'], record['question'])\r\n","    # take all the context start chars then add a final index for the last character\r\n","    sep_index = tokens.ids.index(special_tokens['[SEP]'])\r\n","    offsets = [s for s, _ in tokens.offsets[:sep_index]] + [len(record['context'])]\r\n","    # token boundaries to be used during training are computed\r\n","    start_token, end_token = compute_boundaries(offsets, record['start'], len(record['answer']))\r\n","    # input, output and utility data are returned to form the dataset\r\n","    return [tokens.ids, tokens.type_ids, start_token, end_token, offsets]\r\n","\r\n","  processed_df = pd.DataFrame(\r\n","    [[id] + process_record(record) for id, record in df.iterrows()],\r\n","    columns = ['id', 'ids', 'types', 'start token', 'end token', 'offsets']\r\n","  ).set_index(['id'])\r\n","  return processed_df.join(df)\r\n","\r\n","train_df = process_dataframe(train_df)\r\n","val_df = process_dataframe(val_df)\r\n","\r\n","train_df[['ids', 'types', 'start token', 'end token', 'offsets']]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ids</th>\n","      <th>types</th>\n","      <th>start token</th>\n","      <th>end token</th>\n","      <th>offsets</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5733be284776f41900661182</th>\n","      <td>[101, 6549, 2135, 1010, 1996, 2082, 2038, 1037...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>114</td>\n","      <td>122</td>\n","      <td>[0, 0, 13, 15, 17, 21, 28, 32, 34, 43, 52, 54,...</td>\n","    </tr>\n","    <tr>\n","      <th>5733be284776f4190066117f</th>\n","      <td>[101, 6549, 2135, 1010, 1996, 2082, 2038, 1037...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>40</td>\n","      <td>45</td>\n","      <td>[0, 0, 13, 15, 17, 21, 28, 32, 34, 43, 52, 54,...</td>\n","    </tr>\n","    <tr>\n","      <th>5733be284776f41900661180</th>\n","      <td>[101, 6549, 2135, 1010, 1996, 2082, 2038, 1037...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>65</td>\n","      <td>68</td>\n","      <td>[0, 0, 13, 15, 17, 21, 28, 32, 34, 43, 52, 54,...</td>\n","    </tr>\n","    <tr>\n","      <th>5733be284776f41900661181</th>\n","      <td>[101, 6549, 2135, 1010, 1996, 2082, 2038, 1037...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>85</td>\n","      <td>92</td>\n","      <td>[0, 0, 13, 15, 17, 21, 28, 32, 34, 43, 52, 54,...</td>\n","    </tr>\n","    <tr>\n","      <th>5733be284776f4190066117e</th>\n","      <td>[101, 6549, 2135, 1010, 1996, 2082, 2038, 1037...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>20</td>\n","      <td>27</td>\n","      <td>[0, 0, 13, 15, 17, 21, 28, 32, 34, 43, 52, 54,...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>572686e8dd62a815002e8833</th>\n","      <td>[101, 2012, 1996, 2645, 1997, 16405, 4135, 152...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>135</td>\n","      <td>137</td>\n","      <td>[0, 0, 3, 7, 14, 17, 19, 22, 26, 28, 34, 38, 4...</td>\n","    </tr>\n","    <tr>\n","      <th>572686e8dd62a815002e8834</th>\n","      <td>[101, 2012, 1996, 2645, 1997, 16405, 4135, 152...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>73</td>\n","      <td>76</td>\n","      <td>[0, 0, 3, 7, 14, 17, 19, 22, 26, 28, 34, 38, 4...</td>\n","    </tr>\n","    <tr>\n","      <th>572687e3708984140094c8fd</th>\n","      <td>[101, 4406, 2035, 2060, 2329, 2231, 2636, 1010...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>40</td>\n","      <td>44</td>\n","      <td>[0, 0, 7, 11, 17, 25, 36, 43, 45, 49, 57, 62, ...</td>\n","    </tr>\n","    <tr>\n","      <th>572687e3708984140094c8fe</th>\n","      <td>[101, 4406, 2035, 2060, 2329, 2231, 2636, 1010...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>26</td>\n","      <td>32</td>\n","      <td>[0, 0, 7, 11, 17, 25, 36, 43, 45, 49, 57, 62, ...</td>\n","    </tr>\n","    <tr>\n","      <th>572687e3708984140094c8ff</th>\n","      <td>[101, 4406, 2035, 2060, 2329, 2231, 2636, 1010...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>55</td>\n","      <td>68</td>\n","      <td>[0, 0, 7, 11, 17, 25, 36, 43, 45, 49, 57, 62, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>42765 rows × 5 columns</p>\n","</div>"],"text/plain":["                                                                        ids  ...                                            offsets\n","id                                                                           ...                                                   \n","5733be284776f41900661182  [101, 6549, 2135, 1010, 1996, 2082, 2038, 1037...  ...  [0, 0, 13, 15, 17, 21, 28, 32, 34, 43, 52, 54,...\n","5733be284776f4190066117f  [101, 6549, 2135, 1010, 1996, 2082, 2038, 1037...  ...  [0, 0, 13, 15, 17, 21, 28, 32, 34, 43, 52, 54,...\n","5733be284776f41900661180  [101, 6549, 2135, 1010, 1996, 2082, 2038, 1037...  ...  [0, 0, 13, 15, 17, 21, 28, 32, 34, 43, 52, 54,...\n","5733be284776f41900661181  [101, 6549, 2135, 1010, 1996, 2082, 2038, 1037...  ...  [0, 0, 13, 15, 17, 21, 28, 32, 34, 43, 52, 54,...\n","5733be284776f4190066117e  [101, 6549, 2135, 1010, 1996, 2082, 2038, 1037...  ...  [0, 0, 13, 15, 17, 21, 28, 32, 34, 43, 52, 54,...\n","...                                                                     ...  ...                                                ...\n","572686e8dd62a815002e8833  [101, 2012, 1996, 2645, 1997, 16405, 4135, 152...  ...  [0, 0, 3, 7, 14, 17, 19, 22, 26, 28, 34, 38, 4...\n","572686e8dd62a815002e8834  [101, 2012, 1996, 2645, 1997, 16405, 4135, 152...  ...  [0, 0, 3, 7, 14, 17, 19, 22, 26, 28, 34, 38, 4...\n","572687e3708984140094c8fd  [101, 4406, 2035, 2060, 2329, 2231, 2636, 1010...  ...  [0, 0, 7, 11, 17, 25, 36, 43, 45, 49, 57, 62, ...\n","572687e3708984140094c8fe  [101, 4406, 2035, 2060, 2329, 2231, 2636, 1010...  ...  [0, 0, 7, 11, 17, 25, 36, 43, 45, 49, 57, 62, ...\n","572687e3708984140094c8ff  [101, 4406, 2035, 2060, 2329, 2231, 2636, 1010...  ...  [0, 0, 7, 11, 17, 25, 36, 43, 45, 49, 57, 62, ...\n","\n","[42765 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"tJGhs8-JS7h7"},"source":["### ***1.2. Correctness Check***\r\n","\r\n","Once the dataframe is ready, we check that whether our tokenization is good enough to let us retrieve the correct answers from the text or not.\r\n","\r\n","Answers are retrived by:\r\n","1. getting the two `start` and `end` (token) boundaries that should be computed by the model\r\n","2. converting them into a `start_char` and an `end_char` pair of indices, which represent the boundaries in the original context, using the `indices` list\r\n","3. selecting the correct portion of the `context` using these two (char) boundaries and strip the obtained substring\r\n","\r\n","Some of the answers are not correct, but this is due to the fact that the answers given in the dataset contain substrings or variations of the words which are present in the text."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":490},"id":"_C0zwk5JOBZS","executionInfo":{"status":"ok","timestamp":1610215007432,"user_tz":-60,"elapsed":83675,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"f2438517-66d1-4ca4-e23c-e4959477656f"},"source":["from preprocessing import retrieve_answer, check_correctness\r\n","\r\n","def retrieving_procedure(rec):\r\n","  return retrieve_answer(rec['start token'], rec['end token'], rec['offsets'], rec['context'])\r\n","\r\n","check_correctness(pd.concat((train_df, val_df)), retrieving_procedure)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>answer</th>\n","      <th>normalized answer</th>\n","      <th>retrieved</th>\n","      <th>normalzed retrieved</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>56bf7e603aeaaa14008c9681</th>\n","      <td>split with Luckett and Rober</td>\n","      <td>split with luckett and rober</td>\n","      <td>split with Luckett and Roberson</td>\n","      <td>split with luckett and roberson</td>\n","    </tr>\n","    <tr>\n","      <th>56be973d3aeaaa14008c9123</th>\n","      <td>six</td>\n","      <td>six</td>\n","      <td>sixth</td>\n","      <td>sixth</td>\n","    </tr>\n","    <tr>\n","      <th>5733bc38d058e614000b6188</th>\n","      <td>evolution</td>\n","      <td>evolution</td>\n","      <td>evolutionary</td>\n","      <td>evolutionary</td>\n","    </tr>\n","    <tr>\n","      <th>56cbdea66d243a140015edae</th>\n","      <td>7</td>\n","      <td>7</td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>56cf609aaab44d1400b89187</th>\n","      <td>7</td>\n","      <td>7</td>\n","      <td></td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5727d3462ca10214002d976a</th>\n","      <td>sing</td>\n","      <td>sing</td>\n","      <td>singing</td>\n","      <td>singing</td>\n","    </tr>\n","    <tr>\n","      <th>5727e7654b864d1900163faf</th>\n","      <td>Speak</td>\n","      <td>speak</td>\n","      <td>Speaking</td>\n","      <td>speaking</td>\n","    </tr>\n","    <tr>\n","      <th>572785035951b619008f8c27</th>\n","      <td>19</td>\n","      <td>19</td>\n","      <td>19th</td>\n","      <td>19th</td>\n","    </tr>\n","    <tr>\n","      <th>5728384dff5b5019007d9f4e</th>\n","      <td>ederalism in the United States is the evolving...</td>\n","      <td>ederalism in united states is evolving relatio...</td>\n","      <td>ism in the United States is the evolving relat...</td>\n","      <td>ism in united states is evolving relationship ...</td>\n","    </tr>\n","    <tr>\n","      <th>5728442e2ca10214002da203</th>\n","      <td>8% of public spending, 38% for the regional go...</td>\n","      <td>8 of public spending 38 for regional governmen...</td>\n","      <td>% of public spending, 38% for the regional gov...</td>\n","      <td>of public spending 38 for regional governments...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>192 rows × 4 columns</p>\n","</div>"],"text/plain":["                                                                     answer  ...                                normalzed retrieved\n","id                                                                           ...                                                   \n","56bf7e603aeaaa14008c9681                       split with Luckett and Rober  ...                    split with luckett and roberson\n","56be973d3aeaaa14008c9123                                                six  ...                                              sixth\n","5733bc38d058e614000b6188                                          evolution  ...                                       evolutionary\n","56cbdea66d243a140015edae                                                  7  ...                                                   \n","56cf609aaab44d1400b89187                                                  7  ...                                                   \n","...                                                                     ...  ...                                                ...\n","5727d3462ca10214002d976a                                               sing  ...                                            singing\n","5727e7654b864d1900163faf                                              Speak  ...                                           speaking\n","572785035951b619008f8c27                                                 19  ...                                               19th\n","5728384dff5b5019007d9f4e  ederalism in the United States is the evolving...  ...  ism in united states is evolving relationship ...\n","5728442e2ca10214002da203  8% of public spending, 38% for the regional go...  ...  of public spending 38 for regional governments...\n","\n","[192 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"hEk_uh8wu4-s"},"source":["### ***1.3. Dataset Creation***\r\n","\r\n","We can now prepare the dataset using **Torch** utils for data managing.\r\n","\r\n","* The `Data` class extends *Torch's Dataset* and allows to get input and output data from the dataframe in a lazy way\r\n","> Note that we add the *masks* tensor, which is currently a tensor of ones, that is used by *BERT* to identify which token has to be considered and which one has to be discarded. Indeed, when we will pad the sequences, we will concatenate some *zeros* to this *masks* tensor to represent the padding tokens.\r\n","\r\n","* The `DataLoader`, then, is used to create mini-batches from the dataset and, via the custom function, to pad these mini-batches."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"owigq4r-PBIS","executionInfo":{"status":"ok","timestamp":1610215079515,"user_tz":-60,"elapsed":1020,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"491cb549-10c2-46ab-dfa2-77c8c113a323"},"source":["import torch\r\n","from torch.utils.data import Dataset, DataLoader\r\n","\r\n","class Data(Dataset):\r\n","  def __init__(self, dataframe):\r\n","    self.dataframe = dataframe\r\n","        \r\n","  def __getitem__(self, index):\r\n","    rec = self.dataframe.iloc[index]\r\n","    input_ids = torch.tensor(rec['ids'])\r\n","    input_types = torch.tensor(rec['types'])\r\n","    input_masks = torch.ones_like(input_ids)\r\n","    output_start = torch.tensor(rec['start token'])\r\n","    output_end = torch.tensor(rec['end token'])\r\n","    return (input_ids, input_types, input_masks), (output_start, output_end)\r\n","  \r\n","  def __len__(self):\r\n","    return len(self.dataframe)\r\n","\r\n","train_data = Data(train_df)\r\n","val_data = Data(val_df)\r\n","\r\n","input, output = train_data[0]\r\n","print('Input:')\r\n","print('  > ids:', input[0].shape)\r\n","print('  > types:', input[1].shape)\r\n","print('  > masks:', input[2].shape)\r\n","print('Output:')\r\n","print('  > start:', output[0].shape)\r\n","print('  > end:', output[1].shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input:\n","  > ids: torch.Size([176])\n","  > types: torch.Size([176])\n","  > masks: torch.Size([176])\n","Output:\n","  > start: torch.Size([])\n","  > end: torch.Size([])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rqXB-9JVSktD","executionInfo":{"status":"ok","timestamp":1610215101115,"user_tz":-60,"elapsed":1017,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"5cf35ea7-59d5-49bf-a821-5524a4188742"},"source":["from torch.nn.utils.rnn import pad_sequence\r\n","\r\n","def get_loader(data, batch_size=32):\r\n","  def extract_batch(batch):\r\n","    input_ids = pad_sequence([ii for (ii, _, _), _ in batch], batch_first=True)\r\n","    input_types = pad_sequence([it for (_, it, _), _ in batch], batch_first=True)\r\n","    input_masks = pad_sequence([im for (_, _, im), _ in batch], batch_first=True)\r\n","    output_starts = torch.tensor([os for _, (os, _) in batch])\r\n","    output_ends = torch.tensor([oe for _, (_, oe) in batch])\r\n","    return (input_ids, input_types, input_masks), (output_starts, output_ends)\r\n","  return DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=extract_batch)\r\n","\r\n","for input, output in get_loader(train_data):\r\n","  print('Input:')\r\n","  print('  > ids:', input[0].shape)\r\n","  print('  > types:', input[1].shape)\r\n","  print('  > masks:', input[2].shape)\r\n","  print('Output:')\r\n","  print('  > start:', output[0].shape)\r\n","  print('  > end:', output[1].shape)\r\n","  break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input:\n","  > ids: torch.Size([32, 436])\n","  > types: torch.Size([32, 436])\n","  > masks: torch.Size([32, 436])\n","Output:\n","  > start: torch.Size([32])\n","  > end: torch.Size([32])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vHvyf9SigFCN"},"source":["# **2. Neural Model**\r\n","\r\n","**BERT** is a language model and we will use it as an encoder to produce contextual embeddings for our tokens.\r\n","\r\n","> The model actually returns a dictionary with *two outputs*. One is the `last_hidden_state`, which has shape $[\\mbox{batch_size}, \\mbox{sequence_length}, \\mbox{embedding_dimension}]$, while the other is the `pooler_output`, which has shape $[\\mbox{batch_size}, \\mbox{embedding_dimension}]$. As in our task we want to inspect a sequence to compute the boundaries, we will rely on the first output only."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":117,"referenced_widgets":["6a298f48f28149368d6481da391852b0","c6c5d24eb25f47bea93508a967eb6e23","b282027a522542238bbcd72ee37bc9f0","75330b0d1d2b470eb92bb0b1920235c6","51044f3a924e47a8a17fa4b3afd65d4c","bdcc30a368bd45c1802903415b31f094","8168866578ea478eb3bf9df147db9722","ef6ae12e1fb14832a9100a63433624ca","e38f8d78c9b04bcf9686a006f79671f0","f7284d615851498592deaeeba6cc940c","4a54e4277a4e48b28e1cf38443acc3be","82a38472f76445f0bc197156c357ea75","4b684a865cf84b5793ffd1cbb1c6aefd","89992d0dd12a43a2bc15bbb7b8361aa3","5ee929e4eda5473bb704ad9c699af986","14a32296333b44e3b039e70a7b8ed9da"]},"id":"e8K6NPjabTW5","executionInfo":{"status":"ok","timestamp":1610112159167,"user_tz":-60,"elapsed":11035,"user":{"displayName":"Luca Giuliani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe3b5w2A3KfJdjz-DcdAi2-XnwoAlQ0qo0GDTLyW8=s64","userId":"17994520247536922802"}},"outputId":"5aa1c005-fdc5-4f17-c55d-d37d7b65f3ec"},"source":["from transformers import BertModel\r\n","\r\n","# Pretrained BERT Language Model\r\n","encoder = BertModel.from_pretrained(pretrained_model)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6a298f48f28149368d6481da391852b0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e38f8d78c9b04bcf9686a006f79671f0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8h6LrGZlJfQy"},"source":[""],"execution_count":null,"outputs":[]}]}